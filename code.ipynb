{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad67b543",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T19:02:32.203091Z",
     "iopub.status.busy": "2025-06-09T19:02:32.202803Z",
     "iopub.status.idle": "2025-06-09T19:02:32.623197Z",
     "shell.execute_reply": "2025-06-09T19:02:32.622087Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project structure and API analysis completed. Files have been created in the following directories:\n",
      "data_processing/\n",
      "    README.md\n",
      "    requirements.txt\n",
      "    docs/\n",
      "        api_comparison.md\n",
      "        sample_api_responses.json\n",
      "    market_data/\n",
      "        api_client.py\n",
      "    ml_models/\n",
      "    news_sentiment/\n",
      "    sample_output/\n",
      "\n",
      "API Recommendation:\n",
      "Based on our analysis, TwelveData appears to be the best option for this trading platform due to:\n",
      "1. Comprehensive coverage of all required assets (US100, US30, EUR/USD, GBP/USD, Crude Oil)\n",
      "2. Simple API structure and consistent data formats\n",
      "3. Support for WebSockets for real-time data\n",
      "4. Reasonable pricing plans based on expected usage\n",
      "5. Good documentation and reliability\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up the project directory structure\n",
    "base_dir = '/data/chats/46etsd/workspace/data_processing'\n",
    "subdirs = ['market_data', 'news_sentiment', 'ml_models', 'sample_output', 'docs']\n",
    "\n",
    "for subdir in subdirs:\n",
    "    os.makedirs(os.path.join(base_dir, subdir), exist_ok=True)\n",
    "\n",
    "# API Analysis - We'll evaluate the major financial data APIs and document their capabilities\n",
    "\n",
    "# Initialize a dictionary to store API comparison information\n",
    "api_comparison = {\n",
    "    'TwelveData': {\n",
    "        'description': 'Financial data provider with real-time and historical stock, forex, cryptocurrency, and index data',\n",
    "        'endpoints': {\n",
    "            'time_series': '/time_series - Get OHLCV data',\n",
    "            'quote': '/quote - Get latest price',\n",
    "            'price': '/price - Get real-time price',\n",
    "            'forex_pairs': '/forex_pairs - List available forex pairs',\n",
    "            'indices': '/indices - List available indices'\n",
    "        },\n",
    "        'supported_assets': {\n",
    "            'US100': 'NASDAQ 100 (NDX)',\n",
    "            'US30': 'Dow Jones Industrial Average (DJI)',\n",
    "            'EUR/USD': 'EUR/USD forex pair',\n",
    "            'GBP/USD': 'GBP/USD forex pair',\n",
    "            'Crude Oil': 'WTI and Brent crude oil futures'\n",
    "        },\n",
    "        'pricing': {\n",
    "            'free_tier': '8 API calls/minute, limited endpoints',\n",
    "            'starter': '$12/month - 600 API calls/day',\n",
    "            'standard': '$29/month - 3000 API calls/day',\n",
    "            'premium': '$99/month - 15000 API calls/day'\n",
    "        },\n",
    "        'data_frequency': 'As low as 1 minute on paid plans',\n",
    "        'pros': [\n",
    "            'Comprehensive coverage of all our required assets',\n",
    "            'Simple API structure',\n",
    "            'WebSocket support for real-time data'\n",
    "        ],\n",
    "        'cons': [\n",
    "            'Limited free tier',\n",
    "            'Higher pricing for real-time data'\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    'Finnhub': {\n",
    "        'description': 'Real-time RESTful APIs for stocks, forex, and crypto',\n",
    "        'endpoints': {\n",
    "            'quote': '/quote - Get real-time quote',\n",
    "            'forex/rates': '/forex/rates - Get forex rates',\n",
    "            'forex/candle': '/forex/candle - Get candlestick data',\n",
    "            'news': '/news - Get market news',\n",
    "            'indices/constituents': '/indices/constituents - Get index composition'\n",
    "        },\n",
    "        'supported_assets': {\n",
    "            'US100': 'Via index symbol ^NDX',\n",
    "            'US30': 'Via index symbol ^DJI',\n",
    "            'EUR/USD': 'Via forex endpoint',\n",
    "            'GBP/USD': 'Via forex endpoint',\n",
    "            'Crude Oil': 'Via commodity symbols CL and BZ'\n",
    "        },\n",
    "        'pricing': {\n",
    "            'free_tier': '60 API calls/minute, basic data only',\n",
    "            'basic': '$15/month - 60 API calls/minute, more endpoints',\n",
    "            'standard': '$35/month - 120 API calls/minute, full access',\n",
    "            'premium': '$150/month - 600 API calls/minute, premium data'\n",
    "        },\n",
    "        'data_frequency': '1 minute candles minimum',\n",
    "        'pros': [\n",
    "            'Good news sentiment API included',\n",
    "            'Strong forex and crypto coverage',\n",
    "            'WebSocket support'\n",
    "        ],\n",
    "        'cons': [\n",
    "            'Index data sometimes delayed',\n",
    "            'API structure more complex than TwelveData'\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    'Alpha Vantage': {\n",
    "        'description': 'Provides free APIs for realtime and historical data on stocks, forex, and cryptocurrencies',\n",
    "        'endpoints': {\n",
    "            'TIME_SERIES_INTRADAY': 'Get intraday time series',\n",
    "            'GLOBAL_QUOTE': 'Get current price and volume',\n",
    "            'FX_DAILY': 'Get daily forex rates',\n",
    "            'FX_INTRADAY': 'Get intraday forex data',\n",
    "            'NEWS_SENTIMENT': 'Get news sentiment data'\n",
    "        },\n",
    "        'supported_assets': {\n",
    "            'US100': 'Via index symbol NDX',\n",
    "            'US30': 'Via index symbol DJI',\n",
    "            'EUR/USD': 'Via forex endpoint EUR/USD',\n",
    "            'GBP/USD': 'Via forex endpoint GBP/USD',\n",
    "            'Crude Oil': 'Limited support through commodity ETFs like USO'\n",
    "        },\n",
    "        'pricing': {\n",
    "            'free_tier': '5 API calls/minute, 500/day',\n",
    "            'premium': '$50/month - 120 API calls/minute, 5000 API calls/day',\n",
    "            'enterprise': 'Custom pricing'\n",
    "        },\n",
    "        'data_frequency': 'As low as 1 minute, but with limitations on free tier',\n",
    "        'pros': [\n",
    "            'Generous free tier for prototyping',\n",
    "            'Good documentation',\n",
    "            'CSV format option'\n",
    "        ],\n",
    "        'cons': [\n",
    "            'Rate limits can be restrictive',\n",
    "            'Less comprehensive coverage for indices',\n",
    "            'No WebSocket for real-time data'\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Sample API response structure for each API\n",
    "sample_responses = {\n",
    "    'TwelveData': {\n",
    "        'time_series': {\n",
    "            'meta': {\n",
    "                'symbol': 'EUR/USD',\n",
    "                'interval': '1min',\n",
    "                'currency_base': 'Euro',\n",
    "                'currency_quote': 'US Dollar',\n",
    "                'type': 'Physical Currency'\n",
    "            },\n",
    "            'values': [\n",
    "                {\n",
    "                    'datetime': '2023-01-01 00:00:00',\n",
    "                    'open': '1.0701',\n",
    "                    'high': '1.0702',\n",
    "                    'low': '1.0698',\n",
    "                    'close': '1.0700',\n",
    "                    'volume': '1547'\n",
    "                }\n",
    "            ],\n",
    "            'status': 'ok'\n",
    "        }\n",
    "    },\n",
    "    'Finnhub': {\n",
    "        'quote': {\n",
    "            'c': 148.56,  # Current price\n",
    "            'h': 149.89,  # High price of the day\n",
    "            'l': 147.95,  # Low price of the day\n",
    "            'o': 148.06,  # Open price of the day\n",
    "            'pc': 147.92,  # Previous close price\n",
    "            't': 1698346800  # Unix timestamp\n",
    "        }\n",
    "    },\n",
    "    'Alpha Vantage': {\n",
    "        'Time Series FX (1min)': {\n",
    "            '2023-01-01 00:00:00': {\n",
    "                '1. open': '1.0701',\n",
    "                '2. high': '1.0702',\n",
    "                '3. low': '1.0698',\n",
    "                '4. close': '1.0700'\n",
    "            }\n",
    "        },\n",
    "        'Meta Data': {\n",
    "            '1. Information': 'FX Intraday (1min) Time Series',\n",
    "            '2. From Symbol': 'EUR',\n",
    "            '3. To Symbol': 'USD',\n",
    "            '4. Last Refreshed': '2023-01-01 00:00:00'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Write out the API comparison documentation\n",
    "with open(os.path.join(base_dir, 'docs', 'api_comparison.md'), 'w') as f:\n",
    "    f.write('# Financial Data API Comparison\\n\\n')\n",
    "    f.write('This document compares the main features of financial data APIs for our trading assistant platform.\\n\\n')\n",
    "    \n",
    "    for api_name, details in api_comparison.items():\n",
    "        f.write(f'## {api_name}\\n\\n')\n",
    "        f.write(f'**Description**: {details[\"description\"]}\\n\\n')\n",
    "        \n",
    "        f.write('### Supported Assets\\n')\n",
    "        for asset, desc in details['supported_assets'].items():\n",
    "            f.write(f'- **{asset}**: {desc}\\n')\n",
    "        f.write('\\n')\n",
    "        \n",
    "        f.write('### Key Endpoints\\n')\n",
    "        for endpoint, desc in details['endpoints'].items():\n",
    "            f.write(f'- **{endpoint}**: {desc}\\n')\n",
    "        f.write('\\n')\n",
    "        \n",
    "        f.write('### Pricing Tiers\\n')\n",
    "        for tier, price in details['pricing'].items():\n",
    "            f.write(f'- **{tier}**: {price}\\n')\n",
    "        f.write('\\n')\n",
    "        \n",
    "        f.write('### Data Frequency\\n')\n",
    "        f.write(f'{details[\"data_frequency\"]}\\n\\n')\n",
    "        \n",
    "        f.write('### Pros\\n')\n",
    "        for pro in details['pros']:\n",
    "            f.write(f'- {pro}\\n')\n",
    "        f.write('\\n')\n",
    "        \n",
    "        f.write('### Cons\\n')\n",
    "        for con in details['cons']:\n",
    "            f.write(f'- {con}\\n')\n",
    "        f.write('\\n')\n",
    "        \n",
    "        f.write('---\\n\\n')\n",
    "\n",
    "# Create a sample API response file for reference\n",
    "with open(os.path.join(base_dir, 'docs', 'sample_api_responses.json'), 'w') as f:\n",
    "    json.dump(sample_responses, f, indent=4)\n",
    "\n",
    "# Create a requirements.txt file\n",
    "with open(os.path.join(base_dir, 'requirements.txt'), 'w') as f:\n",
    "    f.write('pandas>=1.3.0\\n')\n",
    "    f.write('requests>=2.25.0\\n')\n",
    "    f.write('matplotlib>=3.4.0\\n')\n",
    "    f.write('scikit-learn>=0.24.0\\n')\n",
    "    f.write('nltk>=3.6.0\\n')\n",
    "    f.write('textblob>=0.15.0\\n')\n",
    "    f.write('vaderSentiment>=3.3.0\\n')\n",
    "    f.write('numpy>=1.20.0\\n')\n",
    "    f.write('joblib>=1.0.0\\n')\n",
    "    f.write('flask>=2.0.0\\n')  # For API endpoints\n",
    "    f.write('websocket-client>=1.0.0\\n')  # For real-time data\n",
    "\n",
    "# Create an initial API integration module\n",
    "with open(os.path.join(base_dir, 'market_data', 'api_client.py'), 'w') as f:\n",
    "    f.write('''\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "class MarketDataClient:\n",
    "    def __init__(self, api_provider='twelvedata', api_key=None):\n",
    "        \"\"\"\n",
    "        Initialize the market data client with the chosen API provider\n",
    "        \n",
    "        Args:\n",
    "            api_provider (str): The API provider to use ('twelvedata', 'finnhub', or 'alphavantage')\n",
    "            api_key (str): API key for the chosen provider\n",
    "        \"\"\"\n",
    "        self.api_provider = api_provider.lower()\n",
    "        self.api_key = api_key\n",
    "        \n",
    "        # Base URLs for each provider\n",
    "        self.base_urls = {\n",
    "            'twelvedata': 'https://api.twelvedata.com',\n",
    "            'finnhub': 'https://finnhub.io/api/v1',\n",
    "            'alphavantage': 'https://www.alphavantage.co/query'\n",
    "        }\n",
    "        \n",
    "        # Symbol mapping for different providers\n",
    "        self.symbol_mapping = {\n",
    "            'US100': {\n",
    "                'twelvedata': 'NDX',\n",
    "                'finnhub': '^NDX',\n",
    "                'alphavantage': 'NDX'\n",
    "            },\n",
    "            'US30': {\n",
    "                'twelvedata': 'DJI',\n",
    "                'finnhub': '^DJI',\n",
    "                'alphavantage': 'DJI'\n",
    "            },\n",
    "            'EUR/USD': {\n",
    "                'twelvedata': 'EUR/USD',\n",
    "                'finnhub': 'EURUSD',\n",
    "                'alphavantage': 'EUR/USD'\n",
    "            },\n",
    "            'GBP/USD': {\n",
    "                'twelvedata': 'GBP/USD',\n",
    "                'finnhub': 'GBPUSD',\n",
    "                'alphavantage': 'GBP/USD'\n",
    "            },\n",
    "            'Crude Oil WTI': {\n",
    "                'twelvedata': 'WTI',\n",
    "                'finnhub': 'CL',\n",
    "                'alphavantage': 'USO'  # Using ETF as proxy\n",
    "            },\n",
    "            'Crude Oil Brent': {\n",
    "                'twelvedata': 'BRENT',\n",
    "                'finnhub': 'BZ',\n",
    "                'alphavantage': 'BNO'  # Using ETF as proxy\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def get_current_price(self, asset):\n",
    "        \"\"\"Get the current price of an asset\n",
    "        \n",
    "        Args:\n",
    "            asset (str): Asset symbol from our standardized list\n",
    "            \n",
    "        Returns:\n",
    "            dict: Current price data\n",
    "        \"\"\"\n",
    "        if self.api_provider not in self.base_urls:\n",
    "            raise ValueError(f\"Unsupported API provider: {self.api_provider}\")\n",
    "            \n",
    "        symbol = self.symbol_mapping.get(asset, {}).get(self.api_provider)\n",
    "        if not symbol:\n",
    "            raise ValueError(f\"Asset {asset} not supported for {self.api_provider}\")\n",
    "            \n",
    "        if self.api_provider == 'twelvedata':\n",
    "            url = f\"{self.base_urls['twelvedata']}/price\"\n",
    "            params = {\n",
    "                'symbol': symbol,\n",
    "                'apikey': self.api_key\n",
    "            }\n",
    "            response = requests.get(url, params=params)\n",
    "            return response.json()\n",
    "            \n",
    "        elif self.api_provider == 'finnhub':\n",
    "            url = f\"{self.base_urls['finnhub']}/quote\"\n",
    "            params = {\n",
    "                'symbol': symbol,\n",
    "                'token': self.api_key\n",
    "            }\n",
    "            response = requests.get(url, params=params)\n",
    "            return response.json()\n",
    "            \n",
    "        elif self.api_provider == 'alphavantage':\n",
    "            is_forex = '/' in asset\n",
    "            \n",
    "            if is_forex:\n",
    "                url = self.base_urls['alphavantage']\n",
    "                from_currency, to_currency = symbol.split('/')\n",
    "                params = {\n",
    "                    'function': 'CURRENCY_EXCHANGE_RATE',\n",
    "                    'from_currency': from_currency,\n",
    "                    'to_currency': to_currency,\n",
    "                    'apikey': self.api_key\n",
    "                }\n",
    "            else:\n",
    "                url = self.base_urls['alphavantage']\n",
    "                params = {\n",
    "                    'function': 'GLOBAL_QUOTE',\n",
    "                    'symbol': symbol,\n",
    "                    'apikey': self.api_key\n",
    "                }\n",
    "                \n",
    "            response = requests.get(url, params=params)\n",
    "            return response.json()\n",
    "            \n",
    "    def get_historical_data(self, asset, interval='1h', count=100):\n",
    "        \"\"\"Get historical OHLCV data\n",
    "        \n",
    "        Args:\n",
    "            asset (str): Asset symbol from our standardized list\n",
    "            interval (str): Time interval (1m, 5m, 15m, 1h, 4h, 1d)\n",
    "            count (int): Number of candles to return\n",
    "            \n",
    "        Returns:\n",
    "            pandas.DataFrame: Historical price data\n",
    "        \"\"\"\n",
    "        # Implementation would vary based on provider\n",
    "        # This is a placeholder for the actual implementation\n",
    "        pass\n",
    "        \n",
    "    def format_response(self, raw_response, asset, data_type='price'):\n",
    "        \"\"\"\n",
    "        Standardize the API response format for frontend consumption\n",
    "        \n",
    "        Args:\n",
    "            raw_response (dict): API response from the provider\n",
    "            asset (str): Asset symbol\n",
    "            data_type (str): Type of data ('price' or 'historical')\n",
    "            \n",
    "        Returns:\n",
    "            dict: Standardized response\n",
    "        \"\"\"\n",
    "        # Implementation would standardize responses across different providers\n",
    "        # This is a placeholder for the actual implementation\n",
    "        pass\n",
    "''')\n",
    "\n",
    "# Create README with setup instructions\n",
    "with open(os.path.join(base_dir, 'README.md'), 'w') as f:\n",
    "    f.write('''# Trading Assistant Platform - Data Processing Pipeline\n",
    "\n",
    "This component handles the data processing for the trading assistant platform, including:\n",
    "\n",
    "1. Real-time market data retrieval\n",
    "2. News sentiment analysis\n",
    "3. ML prediction models for market signals\n",
    "4. Data formatting for frontend consumption\n",
    "\n",
    "## Directory Structure\n",
    "\n",
    "- `/market_data`: API clients and data retrieval modules\n",
    "- `/news_sentiment`: News API integration and sentiment analysis\n",
    "- `/ml_models`: Prediction models and training scripts\n",
    "- `/sample_output`: Example output data formats\n",
    "- `/docs`: Documentation on APIs, data flow, etc.\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "1. Install requirements:\n",
    "   ```\n",
    "   pip install -r requirements.txt\n",
    "   ```\n",
    "\n",
    "2. Set up API keys:\n",
    "   - Create a `.env` file with your API keys:\n",
    "   ```\n",
    "   TWELVEDATA_API_KEY=your_key_here\n",
    "   FINNHUB_API_KEY=your_key_here\n",
    "   ALPHAVANTAGE_API_KEY=your_key_here\n",
    "   NEWS_API_KEY=your_key_here\n",
    "   ```\n",
    "\n",
    "3. Run the data collection service:\n",
    "   ```\n",
    "   python market_data/collector.py\n",
    "   ```\n",
    "\n",
    "4. Run the sentiment analysis service:\n",
    "   ```\n",
    "   python news_sentiment/analyzer.py\n",
    "   ```\n",
    "\n",
    "5. Run the prediction model:\n",
    "   ```\n",
    "   python ml_models/predictor.py\n",
    "   ```\n",
    "\n",
    "## API Decision\n",
    "\n",
    "After analyzing the available options, we recommend using **TwelveData** as our primary data provider for the following reasons:\n",
    "\n",
    "1. Comprehensive coverage of all our required assets (US100, US30, EUR/USD, GBP/USD, Crude Oil)\n",
    "2. Simple and consistent API structure\n",
    "3. WebSocket support for real-time data\n",
    "4. Reasonable pricing for our expected usage\n",
    "\n",
    "As a backup or complementary service, **Finnhub** offers good news sentiment analysis that we can integrate with our own VADER/TextBlob implementation.\n",
    "''')\n",
    "\n",
    "print(\"Project structure and API analysis completed. Files have been created in the following directories:\")\n",
    "for dirpath, dirnames, filenames in os.walk(base_dir):\n",
    "    level = dirpath.replace(base_dir, '').count(os.sep)\n",
    "    indent = ' ' * 4 * level\n",
    "    print(f\"{indent}{os.path.basename(dirpath)}/\")\n",
    "    sub_indent = ' ' * 4 * (level + 1)\n",
    "    for f in filenames:\n",
    "        print(f\"{sub_indent}{f}\")\n",
    "\n",
    "# Conclusion on best API choice\n",
    "print(\"\\nAPI Recommendation:\")\n",
    "print(\"Based on our analysis, TwelveData appears to be the best option for this trading platform due to:\")\n",
    "print(\"1. Comprehensive coverage of all required assets (US100, US30, EUR/USD, GBP/USD, Crude Oil)\")\n",
    "print(\"2. Simple API structure and consistent data formats\")\n",
    "print(\"3. Support for WebSockets for real-time data\")\n",
    "print(\"4. Reasonable pricing plans based on expected usage\")\n",
    "print(\"5. Good documentation and reliability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a660f213",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T19:02:56.720278Z",
     "iopub.status.busy": "2025-06-09T19:02:56.719505Z",
     "iopub.status.idle": "2025-06-09T19:02:56.724968Z",
     "shell.execute_reply": "2025-06-09T19:02:56.724515Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project base directory: /data/chats/46etsd/workspace/data_processing\n",
      "\n",
      "API providers analyzed: ['TwelveData', 'Finnhub', 'Alpha Vantage']\n",
      "\n",
      "Directories created: ['market_data', 'news_sentiment', 'ml_models', 'sample_output', 'docs']\n",
      "\n",
      "Key features of MarketDataClient:\n",
      "- Supported API providers: ['twelvedata', 'finnhub', 'alphavantage']\n",
      "- Supported assets: ['US100', 'US30', 'EUR/USD', 'GBP/USD', 'Crude Oil']\n"
     ]
    }
   ],
   "source": [
    "# Check key project structures\n",
    "print(\"Project base directory:\", base_dir)\n",
    "print(\"\\nAPI providers analyzed:\", list(api_comparison.keys()))\n",
    "print(\"\\nDirectories created:\", subdirs)\n",
    "print(\"\\nKey features of MarketDataClient:\")\n",
    "print(\"- Supported API providers:\", [\"twelvedata\", \"finnhub\", \"alphavantage\"])\n",
    "print(\"- Supported assets:\", list(api_comparison[\"TwelveData\"][\"supported_assets\"].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a80f15f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T19:05:23.182477Z",
     "iopub.status.busy": "2025-06-09T19:05:23.181748Z",
     "iopub.status.idle": "2025-06-09T19:05:23.220075Z",
     "shell.execute_reply": "2025-06-09T19:05:23.219216Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (3302583515.py, line 623)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 623\u001b[0;36m\u001b[0m\n\u001b[0;31m    f.write('''# News Sentiment Analysis Module\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from threading import Timer\n",
    "import re\n",
    "from typing import List, Dict, Any, Union, Tuple\n",
    "\n",
    "# Import sentiment analysis libraries\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Base directory for our project\n",
    "base_dir = '/data/chats/46etsd/workspace/data_processing'\n",
    "news_dir = os.path.join(base_dir, 'news_sentiment')\n",
    "\n",
    "# Ensure NLTK resources are downloaded\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "\n",
    "class NewsAPIClient:\n",
    "    \"\"\"\n",
    "    Client for fetching financial news from various API sources.\n",
    "    Supports NewsAPI, Finnhub, and Alpha Vantage News endpoints.\n",
    "    \"\"\"\n",
    "    def __init__(self, api_provider='newsapi', api_key=None):\n",
    "        \"\"\"\n",
    "        Initialize the news API client.\n",
    "        \n",
    "        Args:\n",
    "            api_provider (str): The API provider to use ('newsapi', 'finnhub', or 'alphavantage')\n",
    "            api_key (str): API key for the chosen provider\n",
    "        \"\"\"\n",
    "        self.api_provider = api_provider.lower()\n",
    "        self.api_key = api_key\n",
    "        \n",
    "        # Base URLs for each provider\n",
    "        self.base_urls = {\n",
    "            'newsapi': 'https://newsapi.org/v2',\n",
    "            'finnhub': 'https://finnhub.io/api/v1',\n",
    "            'alphavantage': 'https://www.alphavantage.co/query'\n",
    "        }\n",
    "        \n",
    "        # Asset to keyword mapping for better news results\n",
    "        self.asset_keywords = {\n",
    "            'US100': ['NASDAQ 100', 'NDX', 'tech stocks', 'technology sector', 'NASDAQ index'],\n",
    "            'US30': ['Dow Jones', 'DJIA', 'Dow 30', 'industrial average', 'blue chip stocks'],\n",
    "            'EUR/USD': ['EURUSD', 'Euro Dollar', 'EUR USD', 'euro forex', 'european currency'],\n",
    "            'GBP/USD': ['GBPUSD', 'British Pound', 'Sterling Dollar', 'GBP USD', 'pound forex'],\n",
    "            'Crude Oil WTI': ['WTI crude', 'oil price', 'crude oil', 'petroleum market', 'oil futures'],\n",
    "            'Crude Oil Brent': ['Brent crude', 'oil price', 'crude oil', 'petroleum market', 'oil futures']\n",
    "        }\n",
    "    \n",
    "    def get_news_for_asset(self, asset: str, days: int = 1, max_items: int = 10) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Fetch recent news articles related to the specified asset.\n",
    "        \n",
    "        Args:\n",
    "            asset (str): Asset symbol from our standardized list\n",
    "            days (int): Number of days to look back\n",
    "            max_items (int): Maximum number of news items to return\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: List of news articles with title, description, url, published_at, and source\n",
    "        \"\"\"\n",
    "        if self.api_provider not in self.base_urls:\n",
    "            raise ValueError(f\"Unsupported API provider: {self.api_provider}\")\n",
    "        \n",
    "        # Get the appropriate keywords for the asset\n",
    "        keywords = self.asset_keywords.get(asset, [asset])\n",
    "        keywords_str = ' OR '.join(keywords)\n",
    "        \n",
    "        # Calculate date range\n",
    "        end_date = datetime.now()\n",
    "        start_date = end_date - timedelta(days=days)\n",
    "        \n",
    "        if self.api_provider == 'newsapi':\n",
    "            url = f\"{self.base_urls['newsapi']}/everything\"\n",
    "            params = {\n",
    "                'q': keywords_str,\n",
    "                'from': start_date.strftime('%Y-%m-%d'),\n",
    "                'to': end_date.strftime('%Y-%m-%d'),\n",
    "                'language': 'en',\n",
    "                'sortBy': 'publishedAt',\n",
    "                'pageSize': max_items,\n",
    "                'apiKey': self.api_key\n",
    "            }\n",
    "            \n",
    "            response = requests.get(url, params=params)\n",
    "            data = response.json()\n",
    "            \n",
    "            if data.get('status') != 'ok':\n",
    "                print(f\"Error fetching news: {data.get('message', 'Unknown error')}\")\n",
    "                return []\n",
    "            \n",
    "            articles = data.get('articles', [])\n",
    "            return [\n",
    "                {\n",
    "                    'title': article.get('title', ''),\n",
    "                    'description': article.get('description', ''),\n",
    "                    'content': article.get('content', ''),\n",
    "                    'url': article.get('url', ''),\n",
    "                    'published_at': article.get('publishedAt', ''),\n",
    "                    'source': article.get('source', {}).get('name', '')\n",
    "                }\n",
    "                for article in articles[:max_items]\n",
    "            ]\n",
    "            \n",
    "        elif self.api_provider == 'finnhub':\n",
    "            url = f\"{self.base_urls['finnhub']}/news\"\n",
    "            \n",
    "            # For forex pairs, use a different approach with Finnhub\n",
    "            if asset in ['EUR/USD', 'GBP/USD']:\n",
    "                category = 'forex'\n",
    "            elif 'Crude Oil' in asset:\n",
    "                category = 'general'  # Finnhub doesn't have a specific commodity category\n",
    "            else:\n",
    "                category = 'general'\n",
    "                \n",
    "            params = {\n",
    "                'category': category,\n",
    "                'token': self.api_key\n",
    "            }\n",
    "            \n",
    "            response = requests.get(url, params=params)\n",
    "            data = response.json()\n",
    "            \n",
    "            # Filter news based on keywords\n",
    "            filtered_news = []\n",
    "            for article in data:\n",
    "                text = (article.get('headline', '') + ' ' + article.get('summary', '')).lower()\n",
    "                if any(keyword.lower() in text for keyword in keywords):\n",
    "                    filtered_news.append({\n",
    "                        'title': article.get('headline', ''),\n",
    "                        'description': article.get('summary', ''),\n",
    "                        'content': article.get('summary', ''),\n",
    "                        'url': article.get('url', ''),\n",
    "                        'published_at': datetime.fromtimestamp(article.get('datetime', 0)).strftime('%Y-%m-%dT%H:%M:%SZ'),\n",
    "                        'source': article.get('source', '')\n",
    "                    })\n",
    "                    \n",
    "                    if len(filtered_news) >= max_items:\n",
    "                        break\n",
    "                        \n",
    "            return filtered_news\n",
    "            \n",
    "        elif self.api_provider == 'alphavantage':\n",
    "            url = self.base_urls['alphavantage']\n",
    "            \n",
    "            # Alpha Vantage uses tickers for news\n",
    "            ticker_mapping = {\n",
    "                'US100': 'NDX',\n",
    "                'US30': 'DJI',\n",
    "                'EUR/USD': 'EUR',\n",
    "                'GBP/USD': 'GBP',\n",
    "                'Crude Oil WTI': 'USO',\n",
    "                'Crude Oil Brent': 'BNO'\n",
    "            }\n",
    "            \n",
    "            ticker = ticker_mapping.get(asset, asset)\n",
    "            \n",
    "            params = {\n",
    "                'function': 'NEWS_SENTIMENT',\n",
    "                'tickers': ticker,\n",
    "                'time_from': start_date.strftime('%Y%m%dT%H%M'),\n",
    "                'limit': max_items,\n",
    "                'apikey': self.api_key\n",
    "            }\n",
    "            \n",
    "            response = requests.get(url, params=params)\n",
    "            data = response.json()\n",
    "            \n",
    "            feed = data.get('feed', [])\n",
    "            return [\n",
    "                {\n",
    "                    'title': article.get('title', ''),\n",
    "                    'description': article.get('summary', ''),\n",
    "                    'content': article.get('summary', ''),\n",
    "                    'url': article.get('url', ''),\n",
    "                    'published_at': article.get('time_published', ''),\n",
    "                    'source': article.get('source', '')\n",
    "                }\n",
    "                for article in feed[:max_items]\n",
    "            ]\n",
    "        \n",
    "        return []\n",
    "\n",
    "\n",
    "class SentimentAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyzes the sentiment of financial news using VADER and TextBlob.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the sentiment analyzer with VADER and TextBlob\"\"\"\n",
    "        self.vader = SentimentIntensityAnalyzer()\n",
    "        \n",
    "        # Augment VADER lexicon with financial terms\n",
    "        financial_terms = {\n",
    "            'bull': 2.0,\n",
    "            'bear': -2.0,\n",
    "            'bullish': 2.0,\n",
    "            'bearish': -2.0,\n",
    "            'rally': 1.5,\n",
    "            'correction': -1.5,\n",
    "            'profit': 1.8,\n",
    "            'loss': -1.8,\n",
    "            'gains': 1.5,\n",
    "            'decline': -1.5,\n",
    "            'upgrade': 1.7,\n",
    "            'downgrade': -1.7,\n",
    "            'outperform': 1.5,\n",
    "            'underperform': -1.5,\n",
    "            'buy': 1.0,\n",
    "            'sell': -1.0,\n",
    "            'hold': 0.0,\n",
    "            'volatile': -0.5,\n",
    "            'crash': -3.0,\n",
    "            'boom': 3.0,\n",
    "            'recession': -2.5,\n",
    "            'growth': 1.8,\n",
    "            'inflation': -1.0,\n",
    "            'deflation': -1.0,\n",
    "        }\n",
    "        \n",
    "        # Update the VADER lexicon\n",
    "        self.vader.lexicon.update(financial_terms)\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean the text by removing URLs, special characters, etc.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text\n",
    "            \n",
    "        Returns:\n",
    "            str: Cleaned text\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # Remove special characters\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def analyze_vader_sentiment(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Analyze text sentiment using VADER.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, float]: VADER sentiment scores (neg, neu, pos, compound)\n",
    "        \"\"\"\n",
    "        cleaned_text = self.clean_text(text)\n",
    "        if not cleaned_text:\n",
    "            return {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
    "            \n",
    "        return self.vader.polarity_scores(cleaned_text)\n",
    "    \n",
    "    def analyze_textblob_sentiment(self, text: str) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Analyze text sentiment using TextBlob.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[float, float]: Polarity (-1 to 1) and subjectivity (0 to 1)\n",
    "        \"\"\"\n",
    "        cleaned_text = self.clean_text(text)\n",
    "        if not cleaned_text:\n",
    "            return (0.0, 0.0)\n",
    "            \n",
    "        blob = TextBlob(cleaned_text)\n",
    "        return (blob.sentiment.polarity, blob.sentiment.subjectivity)\n",
    "    \n",
    "    def get_combined_sentiment(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Get combined sentiment scores from VADER and TextBlob.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, float]: Combined sentiment scores\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return {\n",
    "                'vader_compound': 0.0,\n",
    "                'vader_pos': 0.0,\n",
    "                'vader_neg': 0.0,\n",
    "                'vader_neu': 1.0,\n",
    "                'textblob_polarity': 0.0,\n",
    "                'textblob_subjectivity': 0.0,\n",
    "                'combined_score': 0.0,\n",
    "                'sentiment_label': 'neutral'\n",
    "            }\n",
    "        \n",
    "        # Get VADER sentiment\n",
    "        vader_scores = self.analyze_vader_sentiment(text)\n",
    "        \n",
    "        # Get TextBlob sentiment\n",
    "        textblob_polarity, textblob_subjectivity = self.analyze_textblob_sentiment(text)\n",
    "        \n",
    "        # Combine scores (weighted average)\n",
    "        vader_weight = 0.7\n",
    "        textblob_weight = 0.3\n",
    "        \n",
    "        # Normalize TextBlob polarity to match VADER's compound range\n",
    "        normalized_textblob = textblob_polarity\n",
    "        \n",
    "        # Weighted average\n",
    "        combined_score = (vader_scores['compound'] * vader_weight) + (normalized_textblob * textblob_weight)\n",
    "        \n",
    "        # Determine sentiment label\n",
    "        if combined_score >= 0.05:\n",
    "            sentiment_label = 'positive'\n",
    "        elif combined_score <= -0.05:\n",
    "            sentiment_label = 'negative'\n",
    "        else:\n",
    "            sentiment_label = 'neutral'\n",
    "        \n",
    "        return {\n",
    "            'vader_compound': vader_scores['compound'],\n",
    "            'vader_pos': vader_scores['pos'],\n",
    "            'vader_neg': vader_scores['neg'],\n",
    "            'vader_neu': vader_scores['neu'],\n",
    "            'textblob_polarity': textblob_polarity,\n",
    "            'textblob_subjectivity': textblob_subjectivity,\n",
    "            'combined_score': combined_score,\n",
    "            'sentiment_label': sentiment_label\n",
    "        }\n",
    "\n",
    "\n",
    "class NewsSentimentEngine:\n",
    "    \"\"\"\n",
    "    Main engine for news sentiment analysis that combines fetching news\n",
    "    and analyzing sentiment.\n",
    "    \"\"\"\n",
    "    def __init__(self, news_client: NewsAPIClient, sentiment_analyzer: SentimentAnalyzer):\n",
    "        \"\"\"\n",
    "        Initialize the news sentiment engine.\n",
    "        \n",
    "        Args:\n",
    "            news_client (NewsAPIClient): News API client\n",
    "            sentiment_analyzer (SentimentAnalyzer): Sentiment analyzer\n",
    "        \"\"\"\n",
    "        self.news_client = news_client\n",
    "        self.sentiment_analyzer = sentiment_analyzer\n",
    "        self.news_cache = {}  # Cache for news articles\n",
    "        self.sentiment_cache = {}  # Cache for sentiment scores\n",
    "    \n",
    "    def get_news_with_sentiment(self, asset: str, days: int = 1, max_items: int = 10) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get news articles with sentiment scores for a specific asset.\n",
    "        \n",
    "        Args:\n",
    "            asset (str): Asset symbol\n",
    "            days (int): Number of days to look back\n",
    "            max_items (int): Maximum number of news items to return\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: News articles with sentiment scores\n",
    "        \"\"\"\n",
    "        # Get news articles\n",
    "        news_articles = self.news_client.get_news_for_asset(asset, days, max_items)\n",
    "        \n",
    "        # Calculate sentiment for each article\n",
    "        for article in news_articles:\n",
    "            # Combine title and description for sentiment analysis\n",
    "            text = article.get('title', '') + ' ' + article.get('description', '')\n",
    "            \n",
    "            # Get sentiment scores\n",
    "            sentiment = self.sentiment_analyzer.get_combined_sentiment(text)\n",
    "            \n",
    "            # Add sentiment to article\n",
    "            article['sentiment'] = sentiment\n",
    "        \n",
    "        return news_articles\n",
    "    \n",
    "    def get_aggregated_sentiment(self, asset: str, days: int = 1, max_items: int = 10) -> Dict:\n",
    "        \"\"\"\n",
    "        Get aggregated sentiment for a specific asset.\n",
    "        \n",
    "        Args:\n",
    "            asset (str): Asset symbol\n",
    "            days (int): Number of days to look back\n",
    "            max_items (int): Maximum number of news items to analyze\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Aggregated sentiment scores\n",
    "        \"\"\"\n",
    "        news_with_sentiment = self.get_news_with_sentiment(asset, days, max_items)\n",
    "        \n",
    "        if not news_with_sentiment:\n",
    "            return {\n",
    "                'asset': asset,\n",
    "                'avg_sentiment_score': 0.0,\n",
    "                'sentiment_label': 'neutral',\n",
    "                'sentiment_distribution': {'positive': 0, 'neutral': 0, 'negative': 0},\n",
    "                'news_count': 0,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "        \n",
    "        # Calculate average sentiment\n",
    "        sentiment_scores = [article['sentiment']['combined_score'] for article in news_with_sentiment]\n",
    "        avg_score = sum(sentiment_scores) / len(sentiment_scores)\n",
    "        \n",
    "        # Count sentiment distribution\n",
    "        sentiment_labels = [article['sentiment']['sentiment_label'] for article in news_with_sentiment]\n",
    "        sentiment_distribution = {\n",
    "            'positive': sentiment_labels.count('positive'),\n",
    "            'neutral': sentiment_labels.count('neutral'),\n",
    "            'negative': sentiment_labels.count('negative')\n",
    "        }\n",
    "        \n",
    "        # Determine overall sentiment label\n",
    "        if avg_score >= 0.1:\n",
    "            label = 'positive'\n",
    "        elif avg_score <= -0.1:\n",
    "            label = 'negative'\n",
    "        else:\n",
    "            label = 'neutral'\n",
    "        \n",
    "        return {\n",
    "            'asset': asset,\n",
    "            'avg_sentiment_score': avg_score,\n",
    "            'sentiment_label': label,\n",
    "            'sentiment_distribution': sentiment_distribution,\n",
    "            'news_count': len(news_with_sentiment),\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "    \n",
    "    def get_key_topics(self, asset: str, days: int = 1, max_items: int = 10) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract key topics from news articles for a specific asset.\n",
    "        This is a simplified implementation using frequency-based extraction.\n",
    "        \n",
    "        Args:\n",
    "            asset (str): Asset symbol\n",
    "            days (int): Number of days to look back\n",
    "            max_items (int): Maximum number of news items to analyze\n",
    "            \n",
    "        Returns:\n",
    "            List[str]: Key topics\n",
    "        \"\"\"\n",
    "        news_articles = self.news_client.get_news_for_asset(asset, days, max_items)\n",
    "        \n",
    "        if not news_articles:\n",
    "            return []\n",
    "        \n",
    "        # Combine all text for analysis\n",
    "        all_text = ' '.join([\n",
    "            (article.get('title', '') + ' ' + article.get('description', ''))\n",
    "            for article in news_articles\n",
    "        ])\n",
    "        \n",
    "        # Clean text\n",
    "        cleaned_text = self.sentiment_analyzer.clean_text(all_text)\n",
    "        \n",
    "        # Simple frequency-based extraction\n",
    "        # In a real implementation, we would use more sophisticated NLP techniques\n",
    "        common_words = ['the', 'a', 'an', 'is', 'are', 'was', 'were', 'and', 'or', \n",
    "                        'but', 'in', 'on', 'at', 'to', 'for', 'by', 'of', 'with']\n",
    "        \n",
    "        words = cleaned_text.lower().split()\n",
    "        word_freq = {}\n",
    "        \n",
    "        for word in words:\n",
    "            if len(word) > 3 and word not in common_words:\n",
    "                word_freq[word] = word_freq.get(word, 0) + 1\n",
    "        \n",
    "        # Get top topics\n",
    "        topics = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        return [topic[0] for topic in topics]\n",
    "    \n",
    "    def save_to_file(self, data, asset: str, data_type: str):\n",
    "        \"\"\"\n",
    "        Save data to file.\n",
    "        \n",
    "        Args:\n",
    "            data: Data to save\n",
    "            asset (str): Asset symbol\n",
    "            data_type (str): Type of data ('news', 'sentiment', 'topics')\n",
    "        \"\"\"\n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.join(news_dir, 'data'), exist_ok=True)\n",
    "        \n",
    "        # Create filename with timestamp\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filename = f\"{asset.replace('/', '_')}_{data_type}_{timestamp}.json\"\n",
    "        filepath = os.path.join(news_dir, 'data', filename)\n",
    "        \n",
    "        # Save to file\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "        \n",
    "        return filepath\n",
    "\n",
    "\n",
    "# Create a sample module for demonstration purposes\n",
    "if __name__ == \"__main__\":\n",
    "    # For demonstration, we'll use mock data to avoid API key requirements\n",
    "    def run_demo():\n",
    "        # Sample news articles for demonstration\n",
    "        sample_news = [\n",
    "            {\n",
    "                \"title\": \"US Dollar Strengthens Against Euro as Fed Signals Rate Hike\",\n",
    "                \"description\": \"The USD gained against the EUR following the Federal Reserve's announcement indicating a potential interest rate increase next month.\",\n",
    "                \"content\": \"The USD gained against the EUR following the Federal Reserve's announcement indicating a potential interest rate increase next month. Analysts suggest this could further strengthen the dollar in the coming weeks.\",\n",
    "                \"url\": \"https://example.com/news/1\",\n",
    "                \"published_at\": \"2023-05-15T14:30:00Z\",\n",
    "                \"source\": \"Financial Times\"\n",
    "            },\n",
    "            {\n",
    "                \"title\": \"Tech Stocks Rally Pushes NASDAQ 100 to New Heights\",\n",
    "                \"description\": \"The NASDAQ 100 reached an all-time high today as technology stocks continued their impressive rally led by semiconductor and AI companies.\",\n",
    "                \"content\": \"The NASDAQ 100 reached an all-time high today as technology stocks continued their impressive rally led by semiconductor and AI companies. Investors remain bullish on tech despite valuation concerns.\",\n",
    "                \"url\": \"https://example.com/news/2\",\n",
    "                \"published_at\": \"2023-05-15T16:45:00Z\",\n",
    "                \"source\": \"Wall Street Journal\"\n",
    "            },\n",
    "            {\n",
    "                \"title\": \"Oil Prices Drop on Increased Supply Concerns\",\n",
    "                \"description\": \"Crude oil prices fell by 3% as OPEC+ members consider increasing production amid global economic slowdown fears.\",\n",
    "                \"content\": \"Crude oil prices fell by 3% as OPEC+ members consider increasing production amid global economic slowdown fears. The move comes as inventories show higher than expected levels.\",\n",
    "                \"url\": \"https://example.com/news/3\",\n",
    "                \"published_at\": \"2023-05-15T10:15:00Z\",\n",
    "                \"source\": \"Bloomberg\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Create a mock news client that returns our sample data\n",
    "        class MockNewsClient:\n",
    "            def get_news_for_asset(self, asset, days=1, max_items=10):\n",
    "                # Return appropriate mock news based on asset\n",
    "                if asset == 'EUR/USD':\n",
    "                    return [sample_news[0]]\n",
    "                elif asset == 'US100':\n",
    "                    return [sample_news[1]]\n",
    "                elif 'Crude Oil' in asset:\n",
    "                    return [sample_news[2]]\n",
    "                else:\n",
    "                    return sample_news\n",
    "        \n",
    "        # Create the sentiment analyzer and news engine\n",
    "        sentiment_analyzer = SentimentAnalyzer()\n",
    "        mock_news_client = MockNewsClient()\n",
    "        news_engine = NewsSentimentEngine(mock_news_client, sentiment_analyzer)\n",
    "        \n",
    "        # Run analysis for each asset\n",
    "        assets = ['US100', 'US30', 'EUR/USD', 'GBP/USD', 'Crude Oil WTI', 'Crude Oil Brent']\n",
    "        \n",
    "        results = {}\n",
    "        for asset in assets:\n",
    "            print(f\"Analyzing news sentiment for {asset}...\")\n",
    "            \n",
    "            # Get news with sentiment\n",
    "            news_with_sentiment = news_engine.get_news_with_sentiment(asset)\n",
    "            \n",
    "            # Get aggregated sentiment\n",
    "            agg_sentiment = news_engine.get_aggregated_sentiment(asset)\n",
    "            \n",
    "            # Get key topics\n",
    "            key_topics = news_engine.get_key_topics(asset)\n",
    "            \n",
    "            # Store results\n",
    "            results[asset] = {\n",
    "                'news': news_with_sentiment,\n",
    "                'sentiment': agg_sentiment,\n",
    "                'topics': key_topics\n",
    "            }\n",
    "            \n",
    "            # Save to file\n",
    "            for data_type, data in results[asset].items():\n",
    "                filepath = news_engine.save_to_file(data, asset, data_type)\n",
    "                print(f\"Saved {data_type} data for {asset} to {filepath}\")\n",
    "        \n",
    "        # Create a sample output for frontend integration\n",
    "        sample_output = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'assets': {}\n",
    "        }\n",
    "        \n",
    "        for asset, data in results.items():\n",
    "            sample_output['assets'][asset] = {\n",
    "                'sentiment_score': data['sentiment']['avg_sentiment_score'],\n",
    "                'sentiment_label': data['sentiment']['sentiment_label'],\n",
    "                'key_topics': data['topics'],\n",
    "                'news_count': data['sentiment']['news_count'],\n",
    "                'latest_headlines': [\n",
    "                    {'title': news['title'], 'sentiment': news['sentiment']['sentiment_label']}\n",
    "                    for news in data['news'][:3]\n",
    "                ]\n",
    "            }\n",
    "        \n",
    "        # Save sample output for frontend integration\n",
    "        os.makedirs(os.path.join(base_dir, 'sample_output'), exist_ok=True)\n",
    "        with open(os.path.join(base_dir, 'sample_output', 'sentiment_data.json'), 'w') as f:\n",
    "            json.dump(sample_output, f, indent=4)\n",
    "        \n",
    "        print(\"\\nDemo completed. Sample output saved for frontend integration.\")\n",
    "\n",
    "    # Run the demo\n",
    "    run_demo()\n",
    "    \n",
    "    # Create a README for the news sentiment module\n",
    "    with open(os.path.join(news_dir, 'README.md'), 'w') as f:\n",
    "        f.write('''# News Sentiment Analysis Module\n",
    "\n",
    "This module provides sentiment analysis for financial news related to trading assets.\n",
    "\n",
    "## Features\n",
    "\n",
    "- Fetches news from various API providers (NewsAPI, Finnhub, Alpha Vantage)\n",
    "- Analyzes sentiment using both VADER and TextBlob\n",
    "- Provides combined sentiment scores with financial domain adaptations\n",
    "- Extracts key topics from news articles\n",
    "- Caches results to minimize API calls\n",
    "- Outputs standardized data for frontend consumption\n",
    "\n",
    "## Usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0469d135",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T19:08:01.137892Z",
     "iopub.status.busy": "2025-06-09T19:08:01.137280Z",
     "iopub.status.idle": "2025-06-09T19:08:01.142692Z",
     "shell.execute_reply": "2025-06-09T19:08:01.141662Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (4166102312.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    f.write('''# News Sentiment Analysis Module\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(news_dir, 'README.md'), 'w') as f:\n",
    "    f.write('''# News Sentiment Analysis Module\n",
    "\n",
    "This module provides sentiment analysis for financial news related to trading assets.\n",
    "\n",
    "## Features\n",
    "\n",
    "- Fetches news from various API providers (NewsAPI, Finnhub, Alpha Vantage)\n",
    "- Analyzes sentiment using both VADER and TextBlob\n",
    "- Provides combined sentiment scores with financial domain adaptations\n",
    "- Extracts key topics from news articles\n",
    "- Caches results to minimize API calls\n",
    "- Outputs standardized data for frontend consumption\n",
    "\n",
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "249cfc5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T19:10:40.966644Z",
     "iopub.status.busy": "2025-06-09T19:10:40.965944Z",
     "iopub.status.idle": "2025-06-09T19:10:40.972513Z",
     "shell.execute_reply": "2025-06-09T19:10:40.971038Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1524001397.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    f.write('''# News Sentiment Analysis Module\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(news_dir, 'README.md'), 'w') as f:\n",
    "    f.write('''# News Sentiment Analysis Module\n",
    "    ...\n",
    "    ## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7088b39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T19:10:54.180622Z",
     "iopub.status.busy": "2025-06-09T19:10:54.179993Z",
     "iopub.status.idle": "2025-06-09T19:10:54.188118Z",
     "shell.execute_reply": "2025-06-09T19:10:54.187398Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API providers analyzed:\n",
      "- TwelveData\n",
      "- Finnhub\n",
      "- Alpha Vantage\n",
      "\n",
      "Project directory structure:\n",
      "Base directory: /data/chats/46etsd/workspace/data_processing\n",
      "Subdirectories: ['market_data', 'news_sentiment', 'ml_models', 'sample_output', 'docs']\n",
      "\n",
      "Available asset mappings in MarketDataClient:\n",
      "- TwelveData\n",
      "- Finnhub\n",
      "- Alpha Vantage\n",
      "\n",
      "API client methods:\n",
      "- def __init__(self, api_provider='twelvedata', api_key=None):\n",
      "- def get_current_price(self, asset):\n",
      "- def get_historical_data(self, asset, interval='1h', count=100):\n",
      "- def format_response(self, raw_response, asset, data_type='price'):\n"
     ]
    }
   ],
   "source": [
    "print(\"API providers analyzed:\")\n",
    "for api_name in api_comparison.keys():\n",
    "    print(f\"- {api_name}\")\n",
    "\n",
    "print(\"\\nProject directory structure:\")\n",
    "print(f\"Base directory: {base_dir}\")\n",
    "print(\"Subdirectories:\", subdirs)\n",
    "\n",
    "print(\"\\nAvailable asset mappings in MarketDataClient:\")\n",
    "for asset in sample_responses.keys():\n",
    "    print(f\"- {asset}\")\n",
    "\n",
    "# Check the content of the API client to understand its functionality\n",
    "import os\n",
    "with open(os.path.join(base_dir, 'market_data', 'api_client.py'), 'r') as f:\n",
    "    api_client_code = f.read()\n",
    "    print(\"\\nAPI client methods:\")\n",
    "    for line in api_client_code.split('\\n'):\n",
    "        if \"def \" in line and \"self\" in line:\n",
    "            print(f\"- {line.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b15b4bba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T19:15:07.642709Z",
     "iopub.status.busy": "2025-06-09T19:15:07.642127Z",
     "iopub.status.idle": "2025-06-09T19:15:07.671843Z",
     "shell.execute_reply": "2025-06-09T19:15:07.670992Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2772580159.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    ```python\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "```python\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Define base directory\n",
    "base_dir = '/data/chats/46etsd/workspace/data_processing'\n",
    "news_dir = os.path.join(base_dir, 'news_sentiment')\n",
    "\n",
    "# Create news sentiment class\n",
    "class NewsSentimentAnalyzer:\n",
    "    def __init__(self, news_api_key=None, finnhub_api_key=None):\n",
    "        \"\"\"\n",
    "        Initialize the news sentiment analyzer with API keys\n",
    "        \n",
    "        Args:\n",
    "            news_api_key (str): API key for NewsAPI\n",
    "            finnhub_api_key (str): API key for Finnhub (alternative news source)\n",
    "        \"\"\"\n",
    "        self.news_api_key = news_api_key\n",
    "        self.finnhub_api_key = finnhub_api_key\n",
    "        \n",
    "        # Download necessary NLTK resources if not already available\n",
    "        try:\n",
    "            nltk.data.find('tokenizers/punkt')\n",
    "        except LookupError:\n",
    "            nltk.download('punkt')\n",
    "            \n",
    "        try:\n",
    "            nltk.data.find('corpora/stopwords')\n",
    "        except LookupError:\n",
    "            nltk.download('stopwords')\n",
    "            \n",
    "        # Initialize sentiment analyzers\n",
    "        self.vader = SentimentIntensityAnalyzer()\n",
    "        \n",
    "        # Asset keyword mapping for news search\n",
    "        self.asset_keywords = {\n",
    "            'US100': ['NASDAQ', 'NASDAQ 100', 'NDX', 'tech stocks', 'technology sector'],\n",
    "            'US30': ['Dow Jones', 'DJIA', 'DJI', 'Dow 30', 'industrial average'],\n",
    "            'EUR/USD': ['EUR/USD', 'Euro Dollar', 'Eurozone', 'ECB', 'Federal Reserve', 'forex'],\n",
    "            'GBP/USD': ['GBP/USD', 'British Pound', 'Sterling', 'Bank of England', 'Cable', 'forex'],\n",
    "            'Crude Oil WTI': ['WTI', 'crude oil', 'oil prices', 'OPEC', 'energy market', 'petroleum'],\n",
    "            'Crude Oil Brent': ['Brent', 'crude oil', 'oil prices', 'OPEC', 'energy market', 'petroleum']\n",
    "        }\n",
    "        \n",
    "    def get_news_from_newsapi(self, asset, days=1, max_articles=10):\n",
    "        \"\"\"\n",
    "        Fetch news articles related to a specific asset from NewsAPI\n",
    "        \n",
    "        Args:\n",
    "            asset (str): Asset symbol from our standardized list\n",
    "            days (int): Number of days to look back for news\n",
    "            max_articles (int): Maximum number of articles to return\n",
    "            \n",
    "        Returns:\n",
    "            list: List of news article dictionaries\n",
    "        \"\"\"\n",
    "        if not self.news_api_key:\n",
    "            raise ValueError(\"NewsAPI key is required\")\n",
    "            \n",
    "        # Get relevant keywords for the asset\n",
    "        keywords = self.asset_keywords.get(asset, [asset])\n",
    "        \n",
    "        # Prepare the query string (OR operator between keywords)\n",
    "        query = ' OR '.join([f'\"{keyword}\"' for keyword in keywords])\n",
    "        \n",
    "        # Calculate date range\n",
    "        to_date = datetime.now().strftime('%Y-%m-%d')\n",
    "        from_date = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Make the API request\n",
    "        url = 'https://newsapi.org/v2/everything'\n",
    "        params = {\n",
    "            'q': query,\n",
    "            'from': from_date,\n",
    "            'to': to_date,\n",
    "            'language': 'en',\n",
    "            'sortBy': 'publishedAt',\n",
    "            'pageSize': max_articles,\n",
    "            'apiKey': self.news_api_key\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, params=params)\n",
    "            data = response.json()\n",
    "            \n",
    "            if data.get('status') != 'ok':\n",
    "                print(f\"NewsAPI error: {data.get('message', 'Unknown error')}\")\n",
    "                return []\n",
    "                \n",
    "            articles = data.get('articles', [])\n",
    "            \n",
    "            # Process and clean the articles\n",
    "            processed_articles = []\n",
    "            for article in articles:\n",
    "                processed_articles.append({\n",
    "                    'title': article.get('title', ''),\n",
    "                    'description': article.get('description', ''),\n",
    "                    'content': article.get('content', ''),\n",
    "                    'url': article.get('url', ''),\n",
    "                    'source': article.get('source', {}).get('name', ''),\n",
    "                    'published_at': article.get('publishedAt', '')\n",
    "                })\n",
    "                \n",
    "            return processed_articles\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching news from NewsAPI: {e}\")\n",
    "            return []\n",
    "            \n",
    "    def get_news_from_finnhub(self, asset, days=1, max_articles=10):\n",
    "        \"\"\"\n",
    "        Fetch news articles related to a specific asset from Finnhub\n",
    "        \n",
    "        Args:\n",
    "            asset (str): Asset symbol from our standardized list\n",
    "            days (int): Number of days to look back for news\n",
    "            max_articles (int): Maximum number of articles to return\n",
    "            \n",
    "        Returns:\n",
    "            list: List of news article dictionaries\n",
    "        \"\"\"\n",
    "        if not self.finnhub_api_key:\n",
    "            raise ValueError(\"Finnhub API key is required\")\n",
    "            \n",
    "        # Map our asset to Finnhub symbol\n",
    "        asset_map = {\n",
    "            'US100': 'COMP.INDX',  # Nasdaq Composite Index\n",
    "            'US30': 'DJI.INDX',    # Dow Jones Industrial Average\n",
    "            'EUR/USD': 'OANDA:EUR_USD',\n",
    "            'GBP/USD': 'OANDA:GBP_USD',\n",
    "            'Crude Oil WTI': 'COMM:WTI',\n",
    "            'Crude Oil Brent': 'COMM:BRENT'\n",
    "        }\n",
    "        \n",
    "        symbol = asset_map.get(asset, '')\n",
    "        \n",
    "        # Calculate date range (Unix timestamp)\n",
    "        to_date = int(datetime.now().timestamp())\n",
    "        from_date = int((datetime.now() - timedelta(days=days)).timestamp())\n",
    "        \n",
    "        # Make the API request\n",
    "        url = 'https://finnhub.io/api/v1/company-news'\n",
    "        params = {\n",
    "            'symbol': symbol,\n",
    "            'from': datetime.fromtimestamp(from_date).strftime('%Y-%m-%d'),\n",
    "            'to': datetime.fromtimestamp(to_date).strftime('%Y-%m-%d'),\n",
    "            'token': self.finnhub_api_key\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, params=params)\n",
    "            data = response.json()\n",
    "            \n",
    "            # Process and clean the articles\n",
    "            processed_articles = []\n",
    "            for article in data[:max_articles]:\n",
    "                processed_articles.append({\n",
    "                    'title': article.get('headline', ''),\n",
    "                    'description': article.get('summary', ''),\n",
    "                    'content': article.get('summary', ''),  # Finnhub doesn't provide full content\n",
    "                    'url': article.get('url', ''),\n",
    "                    'source': article.get('source', ''),\n",
    "                    'published_at': datetime.fromtimestamp(article.get('datetime', 0)).isoformat()\n",
    "                })\n",
    "                \n",
    "            return processed_articles\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching news from Finnhub: {e}\")\n",
    "            return []\n",
    "            \n",
    "    def analyze_sentiment_vader(self, text):\n",
    "        \"\"\"\n",
    "        Analyze sentiment of text using VADER\n",
    "        \n",
    "        Args:\n",
    "            text (str): Text to analyze\n",
    "            \n",
    "        Returns:\n",
    "            dict: Sentiment scores\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return {\n",
    "                'compound': 0.0,\n",
    "                'neg': 0.0,\n",
    "                'neu': 0.0,\n",
    "                'pos': 0.0\n",
    "            }\n",
    "            \n",
    "        return self.vader.polarity_scores(text)\n",
    "        \n",
    "    def analyze_sentiment_textblob(self, text):\n",
    "        \"\"\"\n",
    "        Analyze sentiment of text using TextBlob\n",
    "        \n",
    "        Args:\n",
    "            text (str): Text to analyze\n",
    "            \n",
    "        Returns:\n",
    "            dict: Sentiment scores\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return {\n",
    "                'polarity': 0.0,\n",
    "                'subjectivity': 0.0\n",
    "            }\n",
    "            \n",
    "        blob = TextBlob(text)\n",
    "        return {\n",
    "            'polarity': blob.sentiment.polarity,\n",
    "            'subjectivity': blob.sentiment.subjectivity\n",
    "        }\n",
    "        \n",
    "    def extract_keywords(self, text, num_keywords=5):\n",
    "        \"\"\"\n",
    "        Extract key topics/keywords from text\n",
    "        \n",
    "        Args:\n",
    "            text (str): Text to analyze\n",
    "            num_keywords (int): Number of keywords to extract\n",
    "            \n",
    "        Returns:\n",
    "            list: List of keywords\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "            \n",
    "        # Tokenize and clean text\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        # Remove stop words, punctuation, and short words\n",
    "        filtered_tokens = [\n",
    "            word for word in tokens \n",
    "            if word not in stop_words \n",
    "            and re.match(r'^[a-zA-Z]{3,}$', word)\n",
    "        ]\n",
    "        \n",
    "        # Get most common words\n",
    "        word_counts = Counter(filtered_tokens)\n",
    "        return [word for word, count in word_counts.most_common(num_keywords)]\n",
    "        \n",
    "    def analyze_articles(self, articles):\n",
    "        \"\"\"\n",
    "        Analyze sentiment and extract keywords from a list of articles\n",
    "        \n",
    "        Args:\n",
    "            articles (list): List of news article dictionaries\n",
    "            \n",
    "        Returns:\n",
    "            list: List of articles with sentiment analysis\n",
    "        \"\"\"\n",
    "        analyzed_articles = []\n",
    "        \n",
    "        for article in articles:\n",
    "            # Combine title and description for better analysis\n",
    "            analysis_text = f\"{article['title']} {article['description']}\"\n",
    "            \n",
    "            # Get sentiment scores\n",
    "            vader_sentiment = self.analyze_sentiment_vader(analysis_text)\n",
    "            textblob_sentiment = self.analyze_sentiment_textblob(analysis_text)\n",
    "            \n",
    "            # Extract keywords\n",
    "            keywords = self.extract_keywords(analysis_text)\n",
    "            \n",
    "            # Determine sentiment label\n",
    "            if vader_sentiment['compound'] >= 0.05:\n",
    "                sentiment_label = 'positive'\n",
    "            elif vader_sentiment['compound'] <= -0.05:\n",
    "                sentiment_label = 'negative'\n",
    "            else:\n",
    "                sentiment_label = 'neutral'\n",
    "                \n",
    "            # Add analysis to article\n",
    "            analyzed_article = article.copy()\n",
    "            analyzed_article.update({\n",
    "                'vader_sentiment': vader_sentiment,\n",
    "                'textblob_sentiment': textblob_sentiment,\n",
    "                'keywords': keywords,\n",
    "                'sentiment_label': sentiment_label,\n",
    "                'sentiment_score': vader_sentiment['compound']  # Use compound score as primary metric\n",
    "            })\n",
    "            \n",
    "            analyzed_articles.append(analyzed_article)\n",
    "            \n",
    "        return analyzed_articles\n",
    "        \n",
    "    def get_asset_sentiment_summary(self, asset, days=1, max_articles=10):\n",
    "        \"\"\"\n",
    "        Get overall sentiment summary for an asset\n",
    "        \n",
    "        Args:\n",
    "            asset (str): Asset symbol from our standardized list\n",
    "            days (int): Number of days to look back for news\n",
    "            max_articles (int): Maximum number of articles to return\n",
    "            \n",
    "        Returns:\n",
    "            dict: Sentiment summary statistics\n",
    "        \"\"\"\n",
    "        # Try to get news from NewsAPI first\n",
    "        articles = self.get_news_from_newsapi(asset, days, max_articles)\n",
    "        \n",
    "        # If NewsAPI fails or returns no results, try Finnhub\n",
    "        if not articles and self.finnhub_api_key:\n",
    "            articles = self.get_news_from_finnhub(asset, days, max_articles)\n",
    "            \n",
    "        # If we have articles, analyze them\n",
    "        if articles:\n",
    "            analyzed_articles = self.analyze_articles(articles)\n",
    "            \n",
    "            # Calculate overall sentiment\n",
    "            sentiment_scores = [article['sentiment_score'] for article in analyzed_articles]\n",
    "            avg_sentiment = sum(sentiment_scores) / len(sentiment_scores)\n",
    "            \n",
    "            # Count sentiment labels\n",
    "            sentiment_counts = Counter([article['sentiment_label'] for article in analyzed_articles])\n",
    "            \n",
    "            # Collect all keywords\n",
    "            all_keywords = []\n",
    "            for article in analyzed_articles:\n",
    "                all_keywords.extend(article['keywords'])\n",
    "                \n",
    "            top_keywords = [kw for kw, count in Counter(all_keywords).most_common(5)]\n",
    "            \n",
    "            return {\n",
    "                'asset': asset,\n",
    "                'articles_analyzed': len(analyzed_articles),\n",
    "                'average_sentiment': avg_sentiment,\n",
    "                'sentiment_distribution': dict(sentiment_counts),\n",
    "                'top_keywords': top_keywords,\n",
    "                'sentiment_label': 'positive' if avg_sentiment >= 0.05 else 'negative' if avg_sentiment <= -0.05 else 'neutral',\n",
    "                'articles': analyzed_articles\n",
    "            }\n",
    "        else:\n",
    "            # Return empty summary if no articles found\n",
    "            return {\n",
    "                'asset': asset,\n",
    "                'articles_analyzed': 0,\n",
    "                'average_sentiment': 0.0,\n",
    "                'sentiment_distribution': {'positive': 0, 'neutral': 0, 'negative': 0},\n",
    "                'top_keywords': [],\n",
    "                'sentiment_label': 'neutral',\n",
    "                'articles': []\n",
    "            }\n",
    "            \n",
    "    def generate_sentiment_features(self, asset, days=7):\n",
    "        \"\"\"\n",
    "        Generate sentiment features for ML model\n",
    "        \n",
    "        Args:\n",
    "            asset (str): Asset symbol\n",
    "            days (int): Number of days for historical sentiment\n",
    "            \n",
    "        Returns:\n",
    "            dict: Feature dictionary for ML model\n",
    "        \"\"\"\n",
    "        summary = self.get_asset_sentiment_summary(asset, days=days, max_articles=20)\n",
    "        \n",
    "        # Create features for ML model\n",
    "        features = {\n",
    "            'sentiment_score': summary['average_sentiment'],\n",
    "            'sentiment_positive_ratio': summary['sentiment_distribution'].get('positive', 0) / \n",
    "                                       max(1, summary['articles_analyzed']),\n",
    "            'sentiment_negative_ratio': summary['sentiment_distribution'].get('negative', 0) / \n",
    "                                       max(1, summary['articles_analyzed']),\n",
    "            'article_count': summary['articles_analyzed']\n",
    "        }\n",
    "        \n",
    "        return features\n",
    "\n",
    "# Create example module usage script\n",
    "with open(os.path.join(news_dir, 'analyzer.py'), 'w') as f:\n",
    "    f.write('''\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "from news_sentiment.sentiment_analyzer import NewsSentimentAnalyzer\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "def main():\n",
    "    # Get API keys from environment variables\n",
    "    news_api_key = os.getenv('NEWS_API_KEY')\n",
    "    finnhub_api_key = os.getenv('FINNHUB_API_KEY')\n",
    "    \n",
    "    # Initialize sentiment analyzer\n",
    "    analyzer = NewsSentimentAnalyzer(\n",
    "        news_api_key=news_api_key,\n",
    "        finnhub_api_key=finnhub_api_key\n",
    "    )\n",
    "    \n",
    "    # Define assets to analyze\n",
    "    assets = ['US100', 'US30', 'EUR/USD', 'GBP/USD', 'Crude Oil WTI', 'Crude Oil Brent']\n",
    "    \n",
    "    # Analyze sentiment for each asset\n",
    "    results = {}\n",
    "    for asset in assets:\n",
    "        print(f\"Analyzing sentiment for {asset}...\")\n",
    "        sentiment_summary = analyzer.get_asset_sentiment_summary(asset, days=1)\n",
    "        results[asset] = sentiment_summary\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"  Articles analyzed: {sentiment_summary['articles_analyzed']}\")\n",
    "        print(f\"  Average sentiment: {sentiment_summary['average_sentiment']:.4f}\")\n",
    "        print(f\"  Sentiment label: {sentiment_summary['sentiment_label']}\")\n",
    "        print(f\"  Top keywords: {', '.join(sentiment_summary['top_keywords'])}\")\n",
    "        print()\n",
    "    \n",
    "    # Save results to file\n",
    "    output_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'sample_output')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    output_file = os.path.join(output_dir, f'sentiment_analysis_{timestamp}.json')\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        # Save only the summary part, not the full articles\n",
    "        summary_results = {}\n",
    "        for asset, data in results.items():\n",
    "            summary_results[asset] = {k: v for k, v in data.items() if k != 'articles'}\n",
    "            \n",
    "        json.dump(summary_results, f, indent=2)\n",
    "        \n",
    "    print(f\"Results saved to {output_file}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "''')\n",
    "\n",
    "# Create main sentiment analyzer module file\n",
    "with open(os.path.join(news_dir, 'sentiment_analyzer.py'), 'w') as f:\n",
    "    f.write('''\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "class NewsSentimentAnalyzer:\n",
    "    def __init__(self, news_api_key=None, finnhub_api_key=None):\n",
    "        \"\"\"\n",
    "        Initialize the news sentiment analyzer with API keys\n",
    "        \n",
    "        Args:\n",
    "            news_api_key (str): API key for NewsAPI\n",
    "            finnhub_api_key (str): API key for Finnhub (alternative news source)\n",
    "        \"\"\"\n",
    "        self.news_api_key = news_api_key\n",
    "        self.finnhub_api_key = finnhub_api_key\n",
    "        \n",
    "        # Download necessary NLTK resources if not already available\n",
    "        try:\n",
    "            nltk.data.find('tokenizers/punkt')\n",
    "        except LookupError:\n",
    "            nltk.download('punkt')\n",
    "            \n",
    "        try:\n",
    "            nltk.data.find('corpora/stopwords')\n",
    "        except LookupError:\n",
    "            nltk.download('stopwords')\n",
    "            \n",
    "        # Initialize sentiment analyzers\n",
    "        self.vader = SentimentIntensityAnalyzer()\n",
    "        \n",
    "        # Asset keyword mapping for news search\n",
    "        self.asset_keywords = {\n",
    "            'US100': ['NASDAQ', 'NASDAQ 100', 'NDX', 'tech stocks', 'technology sector'],\n",
    "            'US30': ['Dow Jones', 'DJIA', 'DJI', 'Dow 30', 'industrial average'],\n",
    "            'EUR/USD': ['EUR/USD', 'Euro Dollar', 'Eurozone', 'ECB', 'Federal Reserve', 'forex'],\n",
    "            'GBP/USD': ['GBP/USD', 'British Pound', 'Sterling', 'Bank of England', 'Cable', 'forex'],\n",
    "            'Crude Oil WTI': ['WTI', 'crude oil', 'oil prices', 'OPEC', 'energy market', 'petroleum'],\n",
    "            'Crude Oil Brent': ['Brent', 'crude oil', 'oil prices', 'OPEC', 'energy market', 'petroleum']\n",
    "        }\n",
    "        \n",
    "    def get_news_from_newsapi(self, asset, days=1, max_articles=10):\n",
    "        \"\"\"\n",
    "        Fetch news articles related to a specific asset from NewsAPI\n",
    "        \n",
    "        Args:\n",
    "            asset (str): Asset symbol from our standardized list\n",
    "            days (int): Number of days to look back for news\n",
    "            max_articles (int): Maximum number of articles to return\n",
    "            \n",
    "        Returns:\n",
    "            list: List of news article dictionaries\n",
    "        \"\"\"\n",
    "        if not self.news_api_key:\n",
    "            raise ValueError(\"NewsAPI key is required\")\n",
    "            \n",
    "        # Get relevant keywords for the asset\n",
    "        keywords = self.asset_keywords.get(asset, [asset])\n",
    "        \n",
    "        # Prepare the query string (OR operator between keywords)\n",
    "        query = ' OR '.join([f'\"{keyword}\"' for keyword in keywords])\n",
    "        \n",
    "        # Calculate date range\n",
    "        to_date = datetime.now().strftime('%Y-%m-%d')\n",
    "        from_date = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Make the API request\n",
    "        url = 'https://newsapi.org/v2/everything'\n",
    "        params = {\n",
    "            'q': query,\n",
    "            'from': from_date,\n",
    "            'to': to_date,\n",
    "            'language': 'en',\n",
    "            'sortBy': 'publishedAt',\n",
    "            'pageSize': max_articles,\n",
    "            'apiKey': self.news_api_key\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, params=params)\n",
    "            data = response.json()\n",
    "            \n",
    "            if data.get('status') != 'ok':\n",
    "                print(f\"NewsAPI error: {data.get('message', 'Unknown error')}\")\n",
    "                return []\n",
    "                \n",
    "            articles = data.get('articles', [])\n",
    "            \n",
    "            # Process and clean the articles\n",
    "            processed_articles = []\n",
    "            for article in articles:\n",
    "                processed_articles.append({\n",
    "                    'title': article.get('title', ''),\n",
    "                    'description': article.get('description', ''),\n",
    "                    'content': article.get('content', ''),\n",
    "                    'url': article.get('url', ''),\n",
    "                    'source': article.get('source', {}).get('name', ''),\n",
    "                    'published_at': article.get('publishedAt', '')\n",
    "                })\n",
    "                \n",
    "            return processed_articles\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching news from NewsAPI: {e}\")\n",
    "            return []\n",
    "            \n",
    "    def get_news_from_finnhub(self, asset, days=1, max_articles=10):\n",
    "        \"\"\"\n",
    "        Fetch news articles related to a specific asset from Finnhub\n",
    "        \n",
    "        Args:\n",
    "            asset (str): Asset symbol from our standardized list\n",
    "            days (int): Number of days to look back for news\n",
    "            max_articles (int): Maximum number of articles to return\n",
    "            \n",
    "        Returns:\n",
    "            list: List of news article dictionaries\n",
    "        \"\"\"\n",
    "        if not self.finnhub_api_key:\n",
    "            raise ValueError(\"Finnhub API key is required\")\n",
    "            \n",
    "        # Map our asset to Finnhub symbol\n",
    "        asset_map = {\n",
    "            'US100': 'COMP.INDX',  # Nasdaq Composite Index\n",
    "            'US30': 'DJI.INDX',    # Dow Jones Industrial Average\n",
    "            'EUR/USD': 'OANDA:EUR_USD',\n",
    "            'GBP/USD': 'OANDA:GBP_USD',\n",
    "            'Crude Oil WTI': 'COMM:WTI',\n",
    "            'Crude Oil Brent': 'COMM:BRENT'\n",
    "        }\n",
    "        \n",
    "        symbol = asset_map.get(asset, '')\n",
    "        \n",
    "        # Calculate date range (Unix timestamp)\n",
    "        to_date = int(datetime.now().timestamp())\n",
    "        from_date = int((datetime.now() - timedelta(days=days)).timestamp())\n",
    "        \n",
    "        # Make the API request\n",
    "        url = 'https://finnhub.io/api/v1/company-news'\n",
    "        params = {\n",
    "            'symbol': symbol,\n",
    "            'from': datetime.fromtimestamp(from_date).strftime('%Y-%m-%d'),\n",
    "            'to': datetime.fromtimestamp(to_date).strftime('%Y-%m-%d'),\n",
    "            'token': self.finnhub_api_key\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, params=params)\n",
    "            data = response.json()\n",
    "            \n",
    "            # Process and clean the articles\n",
    "            processed_articles = []\n",
    "            for article in data[:max_articles]:\n",
    "                processed_articles.append({\n",
    "                    'title': article.get('headline', ''),\n",
    "                    'description': article.get('summary', ''),\n",
    "                    'content': article.get('summary', ''),  # Finnhub doesn't provide full content\n",
    "                    'url': article.get('url', ''),\n",
    "                    'source': article.get('source', ''),\n",
    "                    'published_at': datetime.fromtimestamp(article.get('datetime', 0)).isoformat()\n",
    "                })\n",
    "                \n",
    "            return processed_articles\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching news from Finnhub: {e}\")\n",
    "            return []\n",
    "            \n",
    "    def analyze_sentiment_vader(self, text):\n",
    "        \"\"\"\n",
    "        Analyze sentiment of text using VADER\n",
    "        \n",
    "        Args:\n",
    "            text (str): Text to analyze\n",
    "            \n",
    "        Returns:\n",
    "            dict: Sentiment scores\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return {\n",
    "                'compound': 0.0,\n",
    "                'neg': 0.0,\n",
    "                'neu': 0.0,\n",
    "                'pos': 0.0\n",
    "            }\n",
    "            \n",
    "        return self.vader.polarity_scores(text)\n",
    "        \n",
    "    def analyze_sentiment_textblob(self, text):\n",
    "        \"\"\"\n",
    "        Analyze sentiment of text using TextBlob\n",
    "        \n",
    "        Args:\n",
    "            text (str): Text to analyze\n",
    "            \n",
    "        Returns:\n",
    "            dict: Sentiment scores\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return {\n",
    "                'polarity': 0.0,\n",
    "                'subjectivity': 0.0\n",
    "            }\n",
    "            \n",
    "        blob = TextBlob(text)\n",
    "        return {\n",
    "            'polarity': blob.sentiment.polarity,\n",
    "            'subjectivity': blob.sentiment.subjectivity\n",
    "        }\n",
    "        \n",
    "    def extract_keywords(self, text, num_keywords=5):\n",
    "        \"\"\"\n",
    "        Extract key topics/keywords from text\n",
    "        \n",
    "        Args:\n",
    "            text (str): Text to analyze\n",
    "            num_keywords (int): Number of keywords to extract\n",
    "            \n",
    "        Returns:\n",
    "            list: List of keywords\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "            \n",
    "        # Tokenize and clean text\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        # Remove stop words, punctuation, and short words\n",
    "        filtered_tokens = [\n",
    "            word for word in tokens \n",
    "            if word not in stop_words \n",
    "            and re.match(r'^[a-zA-Z]{3,}$', word)\n",
    "        ]\n",
    "        \n",
    "        # Get most common words\n",
    "        word_counts = Counter(filtered_tokens)\n",
    "        return [word for word, count in word_counts.most_common(num_keywords)]\n",
    "        \n",
    "    def analyze_articles(self, articles):\n",
    "        \"\"\"\n",
    "        Analyze sentiment and extract keywords from a list of articles\n",
    "        \n",
    "        Args:\n",
    "            articles (list): List of news article dictionaries\n",
    "            \n",
    "        Returns:\n",
    "            list: List of articles with sentiment analysis\n",
    "        \"\"\"\n",
    "        analyzed_articles = []\n",
    "        \n",
    "        for article in articles:\n",
    "            # Combine title and description for better analysis\n",
    "            analysis_text = f\"{article['title']} {article['description']}\"\n",
    "            \n",
    "            # Get sentiment scores\n",
    "            vader_sentiment = self.analyze_sentiment_vader(analysis_text)\n",
    "            textblob_sentiment = self.analyze_sentiment_textblob(analysis_text)\n",
    "            \n",
    "            # Extract keywords\n",
    "            keywords = self.extract_keywords(analysis_text)\n",
    "            \n",
    "            # Determine sentiment label\n",
    "            if vader_sentiment['compound'] >= 0.05:\n",
    "                sentiment_label = 'positive'\n",
    "            elif vader_sentiment['compound'] <= -0.05:\n",
    "                sentiment_label = 'negative'\n",
    "            else:\n",
    "                sentiment_label = 'neutral'\n",
    "                \n",
    "            # Add analysis to article\n",
    "            analyzed_article = article.copy()\n",
    "            analyzed_article.update({\n",
    "                'vader_sentiment': vader_sentiment,\n",
    "                'textblob_sentiment': textblob_sentiment,\n",
    "                'keywords': keywords,\n",
    "                'sentiment_label': sentiment_label,\n",
    "                'sentiment_score': vader_sentiment['compound']  # Use compound score as primary metric\n",
    "            })\n",
    "            \n",
    "            analyzed_articles.append(analyzed_article)\n",
    "            \n",
    "        return analyzed_articles\n",
    "        \n",
    "    def get_asset_sentiment_summary(self, asset, days=1, max_articles=10):\n",
    "        \"\"\"\n",
    "        Get overall sentiment summary for an asset\n",
    "        \n",
    "        Args:\n",
    "            asset (str): Asset symbol from our standardized list\n",
    "            days (int): Number of days to look back for news\n",
    "            max_articles (int): Maximum number of articles to return\n",
    "            \n",
    "        Returns:\n",
    "            dict: Sentiment summary statistics\n",
    "        \"\"\"\n",
    "        # Try to get news from NewsAPI first\n",
    "        articles = self.get_news_from_newsapi(asset, days, max_articles)\n",
    "        \n",
    "        # If NewsAPI fails or returns no results, try Finnhub\n",
    "        if not articles and self.finnhub_api_key:\n",
    "            articles = self.get_news_from_finnhub(asset, days, max_articles)\n",
    "            \n",
    "        # If we have articles, analyze them\n",
    "        if articles:\n",
    "            analyzed_articles = self.analyze_articles(articles)\n",
    "            \n",
    "            # Calculate overall sentiment\n",
    "            sentiment_scores = [article['sentiment_score'] for article in analyzed_articles]\n",
    "            avg_sentiment = sum(sentiment_scores) / len(sentiment_scores)\n",
    "            \n",
    "            # Count sentiment labels\n",
    "            sentiment_counts = Counter([article['sentiment_label'] for article in analyzed_articles])\n",
    "            \n",
    "            # Collect all keywords\n",
    "            all_keywords = []\n",
    "            for article in analyzed_articles:\n",
    "                all_keywords.extend(article['keywords'])\n",
    "                \n",
    "            top_keywords = [kw for kw, count in Counter(all_keywords).most_common(5)]\n",
    "            \n",
    "            return {\n",
    "                'asset': asset,\n",
    "                'articles_analyzed': len(analyzed_articles),\n",
    "                'average_sentiment': avg_sentiment,\n",
    "                'sentiment_distribution': dict(sentiment_counts),\n",
    "                'top_keywords': top_keywords,\n",
    "                'sentiment_label': 'positive' if avg_sentiment >= 0.05 else 'negative' if avg_sentiment <= -0.05 else 'neutral',\n",
    "                'articles': analyzed_articles\n",
    "            }\n",
    "        else:\n",
    "            # Return empty summary if no articles found\n",
    "            return {\n",
    "                'asset': asset,\n",
    "                'articles_analyzed': 0,\n",
    "                'average_sentiment': 0.0,\n",
    "                'sentiment_distribution': {'positive': 0, 'neutral': 0, 'negative': 0},\n",
    "                'top_keywords': [],\n",
    "                'sentiment_label': 'neutral',\n",
    "                'articles': []\n",
    "            }\n",
    "            \n",
    "    def generate_sentiment_features(self, asset, days=7):\n",
    "        \"\"\"\n",
    "        Generate sentiment features for ML model\n",
    "        \n",
    "        Args:\n",
    "            asset (str): Asset symbol\n",
    "            days (int): Number of days for historical sentiment\n",
    "            \n",
    "        Returns:\n",
    "            dict: Feature dictionary for ML model\n",
    "        \"\"\"\n",
    "        summary = self.get_asset_sentiment_summary(asset, days=days, max_articles=20)\n",
    "        \n",
    "        # Create features for ML model\n",
    "        features = {\n",
    "            'sentiment_score': summary['average_sentiment'],\n",
    "            'sentiment_positive_ratio': summary['sentiment_distribution'].get('positive', 0) / \n",
    "                                       max(1, summary['articles_analyzed']),\n",
    "            'sentiment_negative_ratio': summary['sentiment_distribution'].get('negative', 0) / \n",
    "                                       max(1, summary['articles_analyzed']),\n",
    "            'article_count': summary['articles_analyzed']\n",
    "        }\n",
    "        \n",
    "        return features\n",
    "''')\n",
    "\n",
    "# Create a sample output file to demonstrate the data format\n",
    "sample_data = {\n",
    "    'US100': {\n",
    "        'asset': 'US100',\n",
    "        'articles_analyzed': 10,\n",
    "        'average_sentiment': 0.243,\n",
    "        'sentiment_distribution': {'positive': 6, 'neutral': 3, 'negative': 1},\n",
    "        'top_keywords': ['tech', 'growth', 'nasdaq', 'earnings', 'rally'],\n",
    "        'sentiment_label': 'positive'\n",
    "    },\n",
    "    'EUR/USD': {\n",
    "        'asset': 'EUR/USD',\n",
    "        'articles_analyzed': 8,\n",
    "        'average_sentiment': -0.126,\n",
    "        'sentiment_distribution': {'positive': 2, 'neutral': 3, 'negative': 3},\n",
    "        'top_keywords': ['euro', 'dollar', 'inflation', 'central', 'interest'],\n",
    "        'sentiment_label': 'negative'\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(os.path.join(base_dir, 'sample_output', 'sentiment_sample.json'), 'w') as f:\n",
    "    json.dump(sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9d33a45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T19:17:54.383650Z",
     "iopub.status.busy": "2025-06-09T19:17:54.383126Z",
     "iopub.status.idle": "2025-06-09T19:17:54.698853Z",
     "shell.execute_reply": "2025-06-09T19:17:54.697926Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m json\u001b[38;5;241m.\u001b[39mdump(\u001b[43msample_data\u001b[49m, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_data' is not defined"
     ]
    }
   ],
   "source": [
    "json.dump(sample_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6ca6a1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T19:20:13.295608Z",
     "iopub.status.busy": "2025-06-09T19:20:13.294644Z",
     "iopub.status.idle": "2025-06-09T19:20:13.299858Z",
     "shell.execute_reply": "2025-06-09T19:20:13.298618Z"
    }
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:4\u001b[0;36m\u001b[0m\n\u001b[0;31m    SyntaxError: invalid syntax\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "Cell In[7], line 1\n",
    "       ```python\n",
    "       ^\n",
    "   SyntaxError: invalid syntax\n",
    "   ```\n",
    "   This indicates that I was using a code block format (```python) inside Python code, which is invalid.\n",
    "\n",
    "2. Second, there was a NameError when trying to use `sample_data` and `f` variables:\n",
    "   ```\n",
    "   NameError: name 'sample_data' is not defined\n",
    "   ```\n",
    "   This suggests that the code was split incorrectly and not executed in the proper sequence, causing the variables to go out of scope.\n",
    "\n",
    "The main issue appears to be with the structure of my implementation. I was trying to both define the code and write it to files in the same code block, which led to scope issues. Additionally, I didn't properly close some of the file operations.\n",
    "\n",
    "[improved impl]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2762d8d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T19:20:39.441642Z",
     "iopub.status.busy": "2025-06-09T19:20:39.440464Z",
     "iopub.status.idle": "2025-06-09T19:20:39.447711Z",
     "shell.execute_reply": "2025-06-09T19:20:39.447120Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available API providers:\n",
      "- TwelveData\n",
      "- Finnhub\n",
      "- Alpha Vantage\n",
      "\n",
      "Project directory structure:\n",
      "- /data/chats/46etsd/workspace/data_processing/market_data\n",
      "- /data/chats/46etsd/workspace/data_processing/news_sentiment\n",
      "- /data/chats/46etsd/workspace/data_processing/ml_models\n",
      "- /data/chats/46etsd/workspace/data_processing/sample_output\n",
      "- /data/chats/46etsd/workspace/data_processing/docs\n",
      "\n",
      "Supported assets and their symbols across providers:\n",
      "Asset: US100\n",
      "  - twelvedata: NDX\n",
      "  - finnhub: ^NDX\n",
      "  - alphavantage: NDX\n",
      "Asset: US30\n",
      "  - twelvedata: DJI\n",
      "  - finnhub: ^DJI\n",
      "  - alphavantage: DJI\n",
      "Asset: EUR/USD\n",
      "  - twelvedata: EUR/USD\n",
      "  - finnhub: EURUSD\n",
      "  - alphavantage: EUR/USD\n",
      "Asset: GBP/USD\n",
      "  - twelvedata: GBP/USD\n",
      "  - finnhub: GBPUSD\n",
      "  - alphavantage: GBP/USD\n",
      "Asset: Crude Oil WTI\n",
      "  - twelvedata: WTI\n",
      "  - finnhub: CL\n",
      "  - alphavantage: USO\n",
      "Asset: Crude Oil Brent\n",
      "  - twelvedata: BRENT\n",
      "  - finnhub: BZ\n",
      "  - alphavantage: BNO\n"
     ]
    }
   ],
   "source": [
    "# Print key variables from the completed tasks\n",
    "\n",
    "# Print API comparison data\n",
    "print(\"Available API providers:\")\n",
    "for api_name in api_comparison.keys():\n",
    "    print(f\"- {api_name}\")\n",
    "\n",
    "# Print project directory structure\n",
    "print(\"\\nProject directory structure:\")\n",
    "base_dir = '/data/chats/46etsd/workspace/data_processing'\n",
    "for subdir in subdirs:\n",
    "    print(f\"- {os.path.join(base_dir, subdir)}\")\n",
    "\n",
    "# Print available asset mappings from the MarketDataClient\n",
    "print(\"\\nSupported assets and their symbols across providers:\")\n",
    "# Extract the symbol_mapping from the code\n",
    "symbol_mapping = {\n",
    "    'US100': {\n",
    "        'twelvedata': 'NDX',\n",
    "        'finnhub': '^NDX',\n",
    "        'alphavantage': 'NDX'\n",
    "    },\n",
    "    'US30': {\n",
    "        'twelvedata': 'DJI',\n",
    "        'finnhub': '^DJI',\n",
    "        'alphavantage': 'DJI'\n",
    "    },\n",
    "    'EUR/USD': {\n",
    "        'twelvedata': 'EUR/USD',\n",
    "        'finnhub': 'EURUSD',\n",
    "        'alphavantage': 'EUR/USD'\n",
    "    },\n",
    "    'GBP/USD': {\n",
    "        'twelvedata': 'GBP/USD',\n",
    "        'finnhub': 'GBPUSD',\n",
    "        'alphavantage': 'GBP/USD'\n",
    "    },\n",
    "    'Crude Oil WTI': {\n",
    "        'twelvedata': 'WTI',\n",
    "        'finnhub': 'CL',\n",
    "        'alphavantage': 'USO'\n",
    "    },\n",
    "    'Crude Oil Brent': {\n",
    "        'twelvedata': 'BRENT',\n",
    "        'finnhub': 'BZ',\n",
    "        'alphavantage': 'BNO'\n",
    "    }\n",
    "}\n",
    "for asset, mappings in symbol_mapping.items():\n",
    "    print(f\"Asset: {asset}\")\n",
    "    for provider, symbol in mappings.items():\n",
    "        print(f\"  - {provider}: {symbol}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4bbeaee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T19:23:15.850087Z",
     "iopub.status.busy": "2025-06-09T19:23:15.849216Z",
     "iopub.status.idle": "2025-06-09T19:23:15.879765Z",
     "shell.execute_reply": "2025-06-09T19:23:15.878762Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (346504003.py, line 404)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[12], line 404\u001b[0;36m\u001b[0m\n\u001b[0;31m    f.write('''# News Sentiment Analysis Module\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Set the base directory\n",
    "base_dir = '/data/chats/46etsd/workspace/data_processing'\n",
    "news_dir = os.path.join(base_dir, 'news_sentiment')\n",
    "\n",
    "class NewsAPIClient:\n",
    "    \"\"\"\n",
    "    A client for fetching financial news from various news APIs\n",
    "    \"\"\"\n",
    "    def __init__(self, api_provider='newsapi', api_key=None):\n",
    "        \"\"\"\n",
    "        Initialize the news API client\n",
    "        \n",
    "        Args:\n",
    "            api_provider (str): The news API provider ('newsapi', 'finnhub')\n",
    "            api_key (str): API key for the provider\n",
    "        \"\"\"\n",
    "        self.api_provider = api_provider.lower()\n",
    "        self.api_key = api_key\n",
    "        \n",
    "        # Base URLs for news APIs\n",
    "        self.base_urls = {\n",
    "            'newsapi': 'https://newsapi.org/v2',\n",
    "            'finnhub': 'https://finnhub.io/api/v1'\n",
    "        }\n",
    "        \n",
    "        # Keywords mapping for each asset to use in news search\n",
    "        self.asset_keywords = {\n",
    "            'US100': ['NASDAQ', 'NASDAQ 100', 'NDX', 'tech stocks', 'technology sector'],\n",
    "            'US30': ['Dow Jones', 'Dow 30', 'DJIA', 'Dow Jones Industrial Average'],\n",
    "            'EUR/USD': ['EUR/USD', 'euro dollar', 'euro forex', 'euro currency', 'ECB', 'Federal Reserve'],\n",
    "            'GBP/USD': ['GBP/USD', 'pound dollar', 'sterling', 'Bank of England', 'Brexit'],\n",
    "            'Crude Oil WTI': ['crude oil', 'WTI', 'oil prices', 'OPEC', 'oil market'],\n",
    "            'Crude Oil Brent': ['Brent crude', 'Brent oil', 'oil prices', 'OPEC', 'oil market']\n",
    "        }\n",
    "\n",
    "    def get_news_for_asset(self, asset, days_back=3, max_articles=10):\n",
    "        \"\"\"\n",
    "        Fetch news articles related to a specific asset\n",
    "        \n",
    "        Args:\n",
    "            asset (str): The asset name from our standard list\n",
    "            days_back (int): Number of days to look back for news\n",
    "            max_articles (int): Maximum number of articles to fetch\n",
    "            \n",
    "        Returns:\n",
    "            list: List of news articles\n",
    "        \"\"\"\n",
    "        if self.api_provider not in self.base_urls:\n",
    "            raise ValueError(f\"Unsupported API provider: {self.api_provider}\")\n",
    "            \n",
    "        keywords = self.asset_keywords.get(asset)\n",
    "        if not keywords:\n",
    "            raise ValueError(f\"No keywords defined for asset: {asset}\")\n",
    "            \n",
    "        from_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')\n",
    "        \n",
    "        articles = []\n",
    "        \n",
    "        if self.api_provider == 'newsapi':\n",
    "            # For each keyword, get articles and merge them\n",
    "            for keyword in keywords:\n",
    "                url = f\"{self.base_urls['newsapi']}/everything\"\n",
    "                params = {\n",
    "                    'q': keyword,\n",
    "                    'from': from_date,\n",
    "                    'sortBy': 'publishedAt',\n",
    "                    'language': 'en',\n",
    "                    'apiKey': self.api_key\n",
    "                }\n",
    "                \n",
    "                try:\n",
    "                    response = requests.get(url, params=params)\n",
    "                    if response.status_code == 200:\n",
    "                        data = response.json()\n",
    "                        if data.get('status') == 'ok':\n",
    "                            articles.extend(data.get('articles', []))\n",
    "                    else:\n",
    "                        print(f\"Error fetching news from NewsAPI: {response.status_code}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Exception when fetching news: {e}\")\n",
    "                    \n",
    "        elif self.api_provider == 'finnhub':\n",
    "            # Use Finnhub's news endpoint\n",
    "            for keyword in keywords:\n",
    "                url = f\"{self.base_urls['finnhub']}/news\"\n",
    "                params = {\n",
    "                    'category': 'general',\n",
    "                    'token': self.api_key\n",
    "                }\n",
    "                \n",
    "                try:\n",
    "                    response = requests.get(url, params=params)\n",
    "                    if response.status_code == 200:\n",
    "                        data = response.json()\n",
    "                        # Filter articles containing our keyword\n",
    "                        for article in data:\n",
    "                            if keyword.lower() in article.get('headline', '').lower() or \\\n",
    "                               keyword.lower() in article.get('summary', '').lower():\n",
    "                                articles.append({\n",
    "                                    'title': article.get('headline'),\n",
    "                                    'description': article.get('summary'),\n",
    "                                    'url': article.get('url'),\n",
    "                                    'publishedAt': article.get('datetime'),\n",
    "                                    'source': {'name': article.get('source')}\n",
    "                                })\n",
    "                except Exception as e:\n",
    "                    print(f\"Exception when fetching news: {e}\")\n",
    "        \n",
    "        # Deduplicate articles based on title\n",
    "        unique_articles = []\n",
    "        titles = set()\n",
    "        for article in articles:\n",
    "            if article['title'] not in titles:\n",
    "                titles.add(article['title'])\n",
    "                unique_articles.append(article)\n",
    "                \n",
    "                # Add the asset reference\n",
    "                article['asset'] = asset\n",
    "        \n",
    "        # Sort by publication date and return the most recent ones\n",
    "        sorted_articles = sorted(unique_articles, \n",
    "                                 key=lambda x: x.get('publishedAt', ''),\n",
    "                                 reverse=True)\n",
    "        \n",
    "        return sorted_articles[:max_articles]\n",
    "\n",
    "class SentimentAnalyzer:\n",
    "    \"\"\"\n",
    "    A class for analyzing sentiment of financial news articles\n",
    "    using both VADER and TextBlob\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.vader = SentimentIntensityAnalyzer()\n",
    "        \n",
    "    def analyze_text(self, text):\n",
    "        \"\"\"\n",
    "        Analyze sentiment of a text using both VADER and TextBlob\n",
    "        \n",
    "        Args:\n",
    "            text (str): The text to analyze\n",
    "            \n",
    "        Returns:\n",
    "            dict: Sentiment scores from both analyzers\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return {\n",
    "                'vader_compound': 0,\n",
    "                'vader_pos': 0,\n",
    "                'vader_neg': 0,\n",
    "                'vader_neu': 0,\n",
    "                'textblob_polarity': 0,\n",
    "                'textblob_subjectivity': 0,\n",
    "                'sentiment_label': 'neutral'\n",
    "            }\n",
    "        \n",
    "        # VADER sentiment analysis\n",
    "        vader_scores = self.vader.polarity_scores(text)\n",
    "        \n",
    "        # TextBlob sentiment analysis\n",
    "        tb = TextBlob(text)\n",
    "        textblob_polarity = tb.sentiment.polarity\n",
    "        textblob_subjectivity = tb.sentiment.subjectivity\n",
    "        \n",
    "        # Combined sentiment label\n",
    "        # Use VADER's compound score as the primary indicator\n",
    "        if vader_scores['compound'] >= 0.05:\n",
    "            sentiment_label = 'positive'\n",
    "        elif vader_scores['compound'] <= -0.05:\n",
    "            sentiment_label = 'negative'\n",
    "        else:\n",
    "            sentiment_label = 'neutral'\n",
    "        \n",
    "        return {\n",
    "            'vader_compound': vader_scores['compound'],\n",
    "            'vader_pos': vader_scores['pos'],\n",
    "            'vader_neg': vader_scores['neg'],\n",
    "            'vader_neu': vader_scores['neu'],\n",
    "            'textblob_polarity': textblob_polarity,\n",
    "            'textblob_subjectivity': textblob_subjectivity,\n",
    "            'sentiment_label': sentiment_label\n",
    "        }\n",
    "    \n",
    "    def analyze_article(self, article):\n",
    "        \"\"\"\n",
    "        Analyze sentiment of a news article\n",
    "        \n",
    "        Args:\n",
    "            article (dict): News article dictionary\n",
    "            \n",
    "        Returns:\n",
    "            dict: Original article with added sentiment analysis\n",
    "        \"\"\"\n",
    "        # Create a combined text from title and description\n",
    "        title = article.get('title', '')\n",
    "        description = article.get('description', '')\n",
    "        combined_text = f\"{title}. {description}\"\n",
    "        \n",
    "        # Get sentiment scores\n",
    "        sentiment_scores = self.analyze_text(combined_text)\n",
    "        \n",
    "        # Add to the article dictionary\n",
    "        article.update({\n",
    "            'sentiment': sentiment_scores,\n",
    "            'analyzed_at': datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "        return article\n",
    "    \n",
    "    def extract_key_phrases(self, text, max_phrases=5):\n",
    "        \"\"\"\n",
    "        Extract key phrases from text\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text\n",
    "            max_phrases (int): Maximum number of phrases to extract\n",
    "            \n",
    "        Returns:\n",
    "            list: List of key phrases\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "            \n",
    "        # Simple noun phrase extraction\n",
    "        tb = TextBlob(text)\n",
    "        noun_phrases = list(tb.noun_phrases)\n",
    "        \n",
    "        # Simple keyword extraction based on POS tagging\n",
    "        important_tags = ['NN', 'NNS', 'NNP', 'NNPS', 'JJ']\n",
    "        words = tb.tags  # Get words with POS tags\n",
    "        keywords = [word for word, tag in words if tag in important_tags]\n",
    "        \n",
    "        # Combine phrases and keywords, remove duplicates\n",
    "        all_phrases = noun_phrases + keywords\n",
    "        unique_phrases = list(set(all_phrases))\n",
    "        \n",
    "        # Sort by length (favor longer phrases) and return top N\n",
    "        sorted_phrases = sorted(unique_phrases, key=len, reverse=True)\n",
    "        \n",
    "        return sorted_phrases[:max_phrases]\n",
    "\n",
    "class NewsSentimentManager:\n",
    "    \"\"\"\n",
    "    Manages the collection and analysis of news sentiment for trading assets\n",
    "    \"\"\"\n",
    "    def __init__(self, news_api_client, sentiment_analyzer):\n",
    "        self.news_api_client = news_api_client\n",
    "        self.sentiment_analyzer = sentiment_analyzer\n",
    "        self.data_dir = os.path.join(news_dir, 'data')\n",
    "        os.makedirs(self.data_dir, exist_ok=True)\n",
    "        \n",
    "    def collect_and_analyze_news(self, assets, days_back=3, max_articles_per_asset=10):\n",
    "        \"\"\"\n",
    "        Collect and analyze news for multiple assets\n",
    "        \n",
    "        Args:\n",
    "            assets (list): List of asset names\n",
    "            days_back (int): Days to look back for news\n",
    "            max_articles_per_asset (int): Max articles per asset\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary with asset names as keys and analyzed articles as values\n",
    "        \"\"\"\n",
    "        all_results = {}\n",
    "        \n",
    "        for asset in assets:\n",
    "            print(f\"Collecting news for {asset}...\")\n",
    "            \n",
    "            # Get news articles\n",
    "            articles = self.news_api_client.get_news_for_asset(\n",
    "                asset, days_back, max_articles_per_asset\n",
    "            )\n",
    "            \n",
    "            # Analyze sentiment\n",
    "            analyzed_articles = []\n",
    "            for article in articles:\n",
    "                analyzed_article = self.sentiment_analyzer.analyze_article(article)\n",
    "                analyzed_articles.append(analyzed_article)\n",
    "            \n",
    "            all_results[asset] = analyzed_articles\n",
    "            \n",
    "            # Save to json file\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"{asset.replace('/', '_')}_{timestamp}.json\"\n",
    "            with open(os.path.join(self.data_dir, filename), 'w') as f:\n",
    "                json.dump(analyzed_articles, f, indent=4)\n",
    "                \n",
    "        return all_results\n",
    "    \n",
    "    def calculate_asset_sentiment_summary(self, asset_articles):\n",
    "        \"\"\"\n",
    "        Calculate summary sentiment metrics for an asset\n",
    "        \n",
    "        Args:\n",
    "            asset_articles (list): List of analyzed articles for an asset\n",
    "            \n",
    "        Returns:\n",
    "            dict: Summary sentiment metrics\n",
    "        \"\"\"\n",
    "        if not asset_articles:\n",
    "            return {\n",
    "                'count': 0,\n",
    "                'avg_vader_compound': 0,\n",
    "                'avg_textblob_polarity': 0,\n",
    "                'positive_count': 0,\n",
    "                'negative_count': 0,\n",
    "                'neutral_count': 0,\n",
    "                'sentiment_distribution': {\n",
    "                    'positive': 0,\n",
    "                    'neutral': 0,\n",
    "                    'negative': 0\n",
    "                },\n",
    "                'overall_sentiment': 'neutral',\n",
    "                'key_phrases': []\n",
    "            }\n",
    "            \n",
    "        # Extract sentiment scores\n",
    "        vader_scores = [a['sentiment']['vader_compound'] for a in asset_articles]\n",
    "        textblob_scores = [a['sentiment']['textblob_polarity'] for a in asset_articles]\n",
    "        \n",
    "        # Count sentiments\n",
    "        sentiments = [a['sentiment']['sentiment_label'] for a in asset_articles]\n",
    "        positive_count = sentiments.count('positive')\n",
    "        negative_count = sentiments.count('negative')\n",
    "        neutral_count = sentiments.count('neutral')\n",
    "        total_count = len(sentiments)\n",
    "        \n",
    "        # Calculate distribution percentages\n",
    "        sentiment_dist = {\n",
    "            'positive': (positive_count / total_count) * 100 if total_count > 0 else 0,\n",
    "            'neutral': (neutral_count / total_count) * 100 if total_count > 0 else 0, \n",
    "            'negative': (negative_count / total_count) * 100 if total_count > 0 else 0\n",
    "        }\n",
    "        \n",
    "        # Determine overall sentiment\n",
    "        if positive_count > negative_count and positive_count > neutral_count:\n",
    "            overall = 'positive'\n",
    "        elif negative_count > positive_count and negative_count > neutral_count:\n",
    "            overall = 'negative'\n",
    "        else:\n",
    "            overall = 'neutral'\n",
    "            \n",
    "        # Extract key phrases across all articles\n",
    "        all_text = \" \".join([\n",
    "            f\"{a.get('title', '')}. {a.get('description', '')}\" \n",
    "            for a in asset_articles\n",
    "        ])\n",
    "        key_phrases = self.sentiment_analyzer.extract_key_phrases(all_text, 10)\n",
    "        \n",
    "        return {\n",
    "            'count': total_count,\n",
    "            'avg_vader_compound': np.mean(vader_scores) if vader_scores else 0,\n",
    "            'avg_textblob_polarity': np.mean(textblob_scores) if textblob_scores else 0,\n",
    "            'positive_count': positive_count,\n",
    "            'negative_count': negative_count,\n",
    "            'neutral_count': neutral_count,\n",
    "            'sentiment_distribution': sentiment_dist,\n",
    "            'overall_sentiment': overall,\n",
    "            'key_phrases': key_phrases\n",
    "        }\n",
    "    \n",
    "    def get_news_sentiment_features(self, all_assets_articles):\n",
    "        \"\"\"\n",
    "        Extract sentiment features for ML model\n",
    "        \n",
    "        Args:\n",
    "            all_assets_articles (dict): Dictionary with asset names and analyzed articles\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary with asset names and sentiment features\n",
    "        \"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        for asset, articles in all_assets_articles.items():\n",
    "            # Get summary\n",
    "            summary = self.calculate_asset_sentiment_summary(articles)\n",
    "            \n",
    "            # Extract ML features\n",
    "            features[asset] = {\n",
    "                'sentiment_score': summary['avg_vader_compound'],  # Main sentiment score\n",
    "                'sentiment_magnitude': abs(summary['avg_vader_compound']),  # Intensity of sentiment\n",
    "                'textblob_score': summary['avg_textblob_polarity'],\n",
    "                'positive_ratio': summary['sentiment_distribution']['positive'] / 100,\n",
    "                'negative_ratio': summary['sentiment_distribution']['negative'] / 100,\n",
    "                'neutral_ratio': summary['sentiment_distribution']['neutral'] / 100,\n",
    "                'article_count': summary['count'],\n",
    "                'sentiment_label': summary['overall_sentiment'],\n",
    "                'has_news': summary['count'] > 0\n",
    "            }\n",
    "            \n",
    "        return features\n",
    "\n",
    "# Create a README file for the news sentiment module\n",
    "with open(os.path.join(news_dir, 'README.md'), 'w') as f:\n",
    "    f.write('''# News Sentiment Analysis Module\n",
    "\n",
    "This module handles the collection and analysis of financial news sentiment for trading assets.\n",
    "\n",
    "## Components\n",
    "\n",
    "- `NewsAPIClient`: Fetches news articles from various news API providers\n",
    "- `SentimentAnalyzer`: Analyzes sentiment using VADER and TextBlob\n",
    "- `NewsSentimentManager`: Orchestrates collection, analysis, and storage of sentiment data\n",
    "\n",
    "## Supported Assets\n",
    "\n",
    "- US100 (NASDAQ 100)\n",
    "- US30 (Dow Jones Industrial Average)\n",
    "- EUR/USD (Euro/US Dollar)\n",
    "- GBP/USD (British Pound/US Dollar)\n",
    "- Crude Oil WTI\n",
    "- Crude Oil Brent\n",
    "\n",
    "## Usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc2e7fa8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T19:25:27.013206Z",
     "iopub.status.busy": "2025-06-09T19:25:27.012699Z",
     "iopub.status.idle": "2025-06-09T19:25:27.037689Z",
     "shell.execute_reply": "2025-06-09T19:25:27.036779Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (939305528.py, line 409)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[13], line 409\u001b[0;36m\u001b[0m\n\u001b[0;31m    f.write('''# News Sentiment Analysis Module\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "from metagpt.tools.libs.terminal import Terminal\n",
    "terminal = Terminal()\n",
    "await terminal.run('pip install textblob vaderSentiment')\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Set the base directory\n",
    "base_dir = '/data/chats/46etsd/workspace/data_processing'\n",
    "news_dir = os.path.join(base_dir, 'news_sentiment')\n",
    "os.makedirs(os.path.join(news_dir, 'data'), exist_ok=True)\n",
    "\n",
    "class NewsAPIClient:\n",
    "    \"\"\"\n",
    "    A client for fetching financial news from various news APIs\n",
    "    \"\"\"\n",
    "    def __init__(self, api_provider='newsapi', api_key=None):\n",
    "        \"\"\"\n",
    "        Initialize the news API client\n",
    "        \n",
    "        Args:\n",
    "            api_provider (str): The news API provider ('newsapi', 'finnhub')\n",
    "            api_key (str): API key for the provider\n",
    "        \"\"\"\n",
    "        self.api_provider = api_provider.lower()\n",
    "        self.api_key = api_key\n",
    "        \n",
    "        # Base URLs for news APIs\n",
    "        self.base_urls = {\n",
    "            'newsapi': 'https://newsapi.org/v2',\n",
    "            'finnhub': 'https://finnhub.io/api/v1'\n",
    "        }\n",
    "        \n",
    "        # Keywords mapping for each asset to use in news search\n",
    "        self.asset_keywords = {\n",
    "            'US100': ['NASDAQ', 'NASDAQ 100', 'NDX', 'tech stocks', 'technology sector'],\n",
    "            'US30': ['Dow Jones', 'Dow 30', 'DJIA', 'Dow Jones Industrial Average'],\n",
    "            'EUR/USD': ['EUR/USD', 'euro dollar', 'euro forex', 'euro currency', 'ECB', 'Federal Reserve'],\n",
    "            'GBP/USD': ['GBP/USD', 'pound dollar', 'sterling', 'Bank of England', 'Brexit'],\n",
    "            'Crude Oil WTI': ['crude oil', 'WTI', 'oil prices', 'OPEC', 'oil market'],\n",
    "            'Crude Oil Brent': ['Brent crude', 'Brent oil', 'oil prices', 'OPEC', 'oil market']\n",
    "        }\n",
    "\n",
    "    def get_news_for_asset(self, asset, days_back=3, max_articles=10):\n",
    "        \"\"\"\n",
    "        Fetch news articles related to a specific asset\n",
    "        \n",
    "        Args:\n",
    "            asset (str): The asset name from our standard list\n",
    "            days_back (int): Number of days to look back for news\n",
    "            max_articles (int): Maximum number of articles to fetch\n",
    "            \n",
    "        Returns:\n",
    "            list: List of news articles\n",
    "        \"\"\"\n",
    "        if self.api_provider not in self.base_urls:\n",
    "            raise ValueError(f\"Unsupported API provider: {self.api_provider}\")\n",
    "            \n",
    "        keywords = self.asset_keywords.get(asset)\n",
    "        if not keywords:\n",
    "            raise ValueError(f\"No keywords defined for asset: {asset}\")\n",
    "            \n",
    "        from_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')\n",
    "        \n",
    "        articles = []\n",
    "        \n",
    "        if self.api_provider == 'newsapi':\n",
    "            # For each keyword, get articles and merge them\n",
    "            for keyword in keywords:\n",
    "                url = f\"{self.base_urls['newsapi']}/everything\"\n",
    "                params = {\n",
    "                    'q': keyword,\n",
    "                    'from': from_date,\n",
    "                    'sortBy': 'publishedAt',\n",
    "                    'language': 'en',\n",
    "                    'apiKey': self.api_key\n",
    "                }\n",
    "                \n",
    "                try:\n",
    "                    response = requests.get(url, params=params)\n",
    "                    if response.status_code == 200:\n",
    "                        data = response.json()\n",
    "                        if data.get('status') == 'ok':\n",
    "                            articles.extend(data.get('articles', []))\n",
    "                    else:\n",
    "                        print(f\"Error fetching news from NewsAPI: {response.status_code}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Exception when fetching news: {e}\")\n",
    "                    \n",
    "        elif self.api_provider == 'finnhub':\n",
    "            # Use Finnhub's news endpoint\n",
    "            for keyword in keywords:\n",
    "                url = f\"{self.base_urls['finnhub']}/news\"\n",
    "                params = {\n",
    "                    'category': 'general',\n",
    "                    'token': self.api_key\n",
    "                }\n",
    "                \n",
    "                try:\n",
    "                    response = requests.get(url, params=params)\n",
    "                    if response.status_code == 200:\n",
    "                        data = response.json()\n",
    "                        # Filter articles containing our keyword\n",
    "                        for article in data:\n",
    "                            if keyword.lower() in article.get('headline', '').lower() or \\\n",
    "                               keyword.lower() in article.get('summary', '').lower():\n",
    "                                articles.append({\n",
    "                                    'title': article.get('headline'),\n",
    "                                    'description': article.get('summary'),\n",
    "                                    'url': article.get('url'),\n",
    "                                    'publishedAt': article.get('datetime'),\n",
    "                                    'source': {'name': article.get('source')}\n",
    "                                })\n",
    "                except Exception as e:\n",
    "                    print(f\"Exception when fetching news: {e}\")\n",
    "        \n",
    "        # Deduplicate articles based on title\n",
    "        unique_articles = []\n",
    "        titles = set()\n",
    "        for article in articles:\n",
    "            if article['title'] not in titles:\n",
    "                titles.add(article['title'])\n",
    "                unique_articles.append(article)\n",
    "                \n",
    "                # Add the asset reference\n",
    "                article['asset'] = asset\n",
    "        \n",
    "        # Sort by publication date and return the most recent ones\n",
    "        sorted_articles = sorted(unique_articles, \n",
    "                                 key=lambda x: x.get('publishedAt', ''),\n",
    "                                 reverse=True)\n",
    "        \n",
    "        return sorted_articles[:max_articles]\n",
    "\n",
    "class SentimentAnalyzer:\n",
    "    \"\"\"\n",
    "    A class for analyzing sentiment of financial news articles\n",
    "    using both VADER and TextBlob\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.vader = SentimentIntensityAnalyzer()\n",
    "        \n",
    "    def analyze_text(self, text):\n",
    "        \"\"\"\n",
    "        Analyze sentiment of a text using both VADER and TextBlob\n",
    "        \n",
    "        Args:\n",
    "            text (str): The text to analyze\n",
    "            \n",
    "        Returns:\n",
    "            dict: Sentiment scores from both analyzers\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return {\n",
    "                'vader_compound': 0,\n",
    "                'vader_pos': 0,\n",
    "                'vader_neg': 0,\n",
    "                'vader_neu': 0,\n",
    "                'textblob_polarity': 0,\n",
    "                'textblob_subjectivity': 0,\n",
    "                'sentiment_label': 'neutral'\n",
    "            }\n",
    "        \n",
    "        # VADER sentiment analysis\n",
    "        vader_scores = self.vader.polarity_scores(text)\n",
    "        \n",
    "        # TextBlob sentiment analysis\n",
    "        tb = TextBlob(text)\n",
    "        textblob_polarity = tb.sentiment.polarity\n",
    "        textblob_subjectivity = tb.sentiment.subjectivity\n",
    "        \n",
    "        # Combined sentiment label\n",
    "        # Use VADER's compound score as the primary indicator\n",
    "        if vader_scores['compound'] >= 0.05:\n",
    "            sentiment_label = 'positive'\n",
    "        elif vader_scores['compound'] <= -0.05:\n",
    "            sentiment_label = 'negative'\n",
    "        else:\n",
    "            sentiment_label = 'neutral'\n",
    "        \n",
    "        return {\n",
    "            'vader_compound': vader_scores['compound'],\n",
    "            'vader_pos': vader_scores['pos'],\n",
    "            'vader_neg': vader_scores['neg'],\n",
    "            'vader_neu': vader_scores['neu'],\n",
    "            'textblob_polarity': textblob_polarity,\n",
    "            'textblob_subjectivity': textblob_subjectivity,\n",
    "            'sentiment_label': sentiment_label\n",
    "        }\n",
    "    \n",
    "    def analyze_article(self, article):\n",
    "        \"\"\"\n",
    "        Analyze sentiment of a news article\n",
    "        \n",
    "        Args:\n",
    "            article (dict): News article dictionary\n",
    "            \n",
    "        Returns:\n",
    "            dict: Original article with added sentiment analysis\n",
    "        \"\"\"\n",
    "        # Create a combined text from title and description\n",
    "        title = article.get('title', '')\n",
    "        description = article.get('description', '')\n",
    "        combined_text = f\"{title}. {description}\"\n",
    "        \n",
    "        # Get sentiment scores\n",
    "        sentiment_scores = self.analyze_text(combined_text)\n",
    "        \n",
    "        # Add to the article dictionary\n",
    "        article.update({\n",
    "            'sentiment': sentiment_scores,\n",
    "            'analyzed_at': datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "        return article\n",
    "    \n",
    "    def extract_key_phrases(self, text, max_phrases=5):\n",
    "        \"\"\"\n",
    "        Extract key phrases from text\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text\n",
    "            max_phrases (int): Maximum number of phrases to extract\n",
    "            \n",
    "        Returns:\n",
    "            list: List of key phrases\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "            \n",
    "        # Simple noun phrase extraction\n",
    "        tb = TextBlob(text)\n",
    "        noun_phrases = list(tb.noun_phrases)\n",
    "        \n",
    "        # Simple keyword extraction based on POS tagging\n",
    "        important_tags = ['NN', 'NNS', 'NNP', 'NNPS', 'JJ']\n",
    "        words = tb.tags  # Get words with POS tags\n",
    "        keywords = [word for word, tag in words if tag in important_tags]\n",
    "        \n",
    "        # Combine phrases and keywords, remove duplicates\n",
    "        all_phrases = noun_phrases + keywords\n",
    "        unique_phrases = list(set(all_phrases))\n",
    "        \n",
    "        # Sort by length (favor longer phrases) and return top N\n",
    "        sorted_phrases = sorted(unique_phrases, key=len, reverse=True)\n",
    "        \n",
    "        return sorted_phrases[:max_phrases]\n",
    "\n",
    "class NewsSentimentManager:\n",
    "    \"\"\"\n",
    "    Manages the collection and analysis of news sentiment for trading assets\n",
    "    \"\"\"\n",
    "    def __init__(self, news_api_client, sentiment_analyzer):\n",
    "        self.news_api_client = news_api_client\n",
    "        self.sentiment_analyzer = sentiment_analyzer\n",
    "        self.data_dir = os.path.join(news_dir, 'data')\n",
    "        os.makedirs(self.data_dir, exist_ok=True)\n",
    "        \n",
    "    def collect_and_analyze_news(self, assets, days_back=3, max_articles_per_asset=10):\n",
    "        \"\"\"\n",
    "        Collect and analyze news for multiple assets\n",
    "        \n",
    "        Args:\n",
    "            assets (list): List of asset names\n",
    "            days_back (int): Days to look back for news\n",
    "            max_articles_per_asset (int): Max articles per asset\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary with asset names as keys and analyzed articles as values\n",
    "        \"\"\"\n",
    "        all_results = {}\n",
    "        \n",
    "        for asset in assets:\n",
    "            print(f\"Collecting news for {asset}...\")\n",
    "            \n",
    "            # Get news articles\n",
    "            articles = self.news_api_client.get_news_for_asset(\n",
    "                asset, days_back, max_articles_per_asset\n",
    "            )\n",
    "            \n",
    "            # Analyze sentiment\n",
    "            analyzed_articles = []\n",
    "            for article in articles:\n",
    "                analyzed_article = self.sentiment_analyzer.analyze_article(article)\n",
    "                analyzed_articles.append(analyzed_article)\n",
    "            \n",
    "            all_results[asset] = analyzed_articles\n",
    "            \n",
    "            # Save to json file\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"{asset.replace('/', '_')}_{timestamp}.json\"\n",
    "            with open(os.path.join(self.data_dir, filename), 'w') as f:\n",
    "                json.dump(analyzed_articles, f, indent=4)\n",
    "                \n",
    "        return all_results\n",
    "    \n",
    "    def calculate_asset_sentiment_summary(self, asset_articles):\n",
    "        \"\"\"\n",
    "        Calculate summary sentiment metrics for an asset\n",
    "        \n",
    "        Args:\n",
    "            asset_articles (list): List of analyzed articles for an asset\n",
    "            \n",
    "        Returns:\n",
    "            dict: Summary sentiment metrics\n",
    "        \"\"\"\n",
    "        if not asset_articles:\n",
    "            return {\n",
    "                'count': 0,\n",
    "                'avg_vader_compound': 0,\n",
    "                'avg_textblob_polarity': 0,\n",
    "                'positive_count': 0,\n",
    "                'negative_count': 0,\n",
    "                'neutral_count': 0,\n",
    "                'sentiment_distribution': {\n",
    "                    'positive': 0,\n",
    "                    'neutral': 0,\n",
    "                    'negative': 0\n",
    "                },\n",
    "                'overall_sentiment': 'neutral',\n",
    "                'key_phrases': []\n",
    "            }\n",
    "            \n",
    "        # Extract sentiment scores\n",
    "        vader_scores = [a['sentiment']['vader_compound'] for a in asset_articles]\n",
    "        textblob_scores = [a['sentiment']['textblob_polarity'] for a in asset_articles]\n",
    "        \n",
    "        # Count sentiments\n",
    "        sentiments = [a['sentiment']['sentiment_label'] for a in asset_articles]\n",
    "        positive_count = sentiments.count('positive')\n",
    "        negative_count = sentiments.count('negative')\n",
    "        neutral_count = sentiments.count('neutral')\n",
    "        total_count = len(sentiments)\n",
    "        \n",
    "        # Calculate distribution percentages\n",
    "        sentiment_dist = {\n",
    "            'positive': (positive_count / total_count) * 100 if total_count > 0 else 0,\n",
    "            'neutral': (neutral_count / total_count) * 100 if total_count > 0 else 0, \n",
    "            'negative': (negative_count / total_count) * 100 if total_count > 0 else 0\n",
    "        }\n",
    "        \n",
    "        # Determine overall sentiment\n",
    "        if positive_count > negative_count and positive_count > neutral_count:\n",
    "            overall = 'positive'\n",
    "        elif negative_count > positive_count and negative_count > neutral_count:\n",
    "            overall = 'negative'\n",
    "        else:\n",
    "            overall = 'neutral'\n",
    "            \n",
    "        # Extract key phrases across all articles\n",
    "        all_text = \" \".join([\n",
    "            f\"{a.get('title', '')}. {a.get('description', '')}\" \n",
    "            for a in asset_articles\n",
    "        ])\n",
    "        key_phrases = self.sentiment_analyzer.extract_key_phrases(all_text, 10)\n",
    "        \n",
    "        return {\n",
    "            'count': total_count,\n",
    "            'avg_vader_compound': np.mean(vader_scores) if vader_scores else 0,\n",
    "            'avg_textblob_polarity': np.mean(textblob_scores) if textblob_scores else 0,\n",
    "            'positive_count': positive_count,\n",
    "            'negative_count': negative_count,\n",
    "            'neutral_count': neutral_count,\n",
    "            'sentiment_distribution': sentiment_dist,\n",
    "            'overall_sentiment': overall,\n",
    "            'key_phrases': key_phrases\n",
    "        }\n",
    "    \n",
    "    def get_news_sentiment_features(self, all_assets_articles):\n",
    "        \"\"\"\n",
    "        Extract sentiment features for ML model\n",
    "        \n",
    "        Args:\n",
    "            all_assets_articles (dict): Dictionary with asset names and analyzed articles\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary with asset names and sentiment features\n",
    "        \"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        for asset, articles in all_assets_articles.items():\n",
    "            # Get summary\n",
    "            summary = self.calculate_asset_sentiment_summary(articles)\n",
    "            \n",
    "            # Extract ML features\n",
    "            features[asset] = {\n",
    "                'sentiment_score': summary['avg_vader_compound'],  # Main sentiment score\n",
    "                'sentiment_magnitude': abs(summary['avg_vader_compound']),  # Intensity of sentiment\n",
    "                'textblob_score': summary['avg_textblob_polarity'],\n",
    "                'positive_ratio': summary['sentiment_distribution']['positive'] / 100,\n",
    "                'negative_ratio': summary['sentiment_distribution']['negative'] / 100,\n",
    "                'neutral_ratio': summary['sentiment_distribution']['neutral'] / 100,\n",
    "                'article_count': summary['count'],\n",
    "                'sentiment_label': summary['overall_sentiment'],\n",
    "                'has_news': summary['count'] > 0\n",
    "            }\n",
    "            \n",
    "        return features\n",
    "\n",
    "# Create a README file for the news sentiment module with proper triple-quote closing\n",
    "with open(os.path.join(news_dir, 'README.md'), 'w') as f:\n",
    "    f.write('''# News Sentiment Analysis Module\n",
    "\n",
    "This module handles the collection and analysis of financial news sentiment for trading assets.\n",
    "\n",
    "## Components\n",
    "\n",
    "- `NewsAPIClient`: Fetches news articles from various news API providers\n",
    "- `SentimentAnalyzer`: Analyzes sentiment using VADER and TextBlob\n",
    "- `NewsSentimentManager`: Orchestrates collection, analysis, and storage of sentiment data\n",
    "\n",
    "## Supported Assets\n",
    "\n",
    "- US100 (NASDAQ 100)\n",
    "- US30 (Dow Jones Industrial Average)\n",
    "- EUR/USD (Euro/US Dollar)\n",
    "- GBP/USD (British Pound/US Dollar)\n",
    "- Crude Oil WTI\n",
    "- Crude Oil Brent\n",
    "\n",
    "## Usage\n",
    "\n",
    "### Basic Usage Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d4ba274",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T19:27:47.244719Z",
     "iopub.status.busy": "2025-06-09T19:27:47.244414Z",
     "iopub.status.idle": "2025-06-09T19:27:47.279719Z",
     "shell.execute_reply": "2025-06-09T19:27:47.278519Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (3261422680.py, line 441)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[14], line 441\u001b[0;36m\u001b[0m\n\u001b[0;31m    readme_content = \"\"\"# News Sentiment Analysis Module\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "from metagpt.tools.libs.terminal import Terminal\n",
    "terminal = Terminal()\n",
    "await terminal.run('pip install textblob vaderSentiment')\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Set the base directory\n",
    "base_dir = '/data/chats/46etsd/workspace/data_processing'\n",
    "news_dir = os.path.join(base_dir, 'news_sentiment')\n",
    "os.makedirs(os.path.join(news_dir, 'data'), exist_ok=True)\n",
    "\n",
    "class NewsAPIClient:\n",
    "    \"\"\"\n",
    "    A client for fetching financial news from various news APIs\n",
    "    \"\"\"\n",
    "    def __init__(self, api_provider='newsapi', api_key=None):\n",
    "        \"\"\"\n",
    "        Initialize the news API client\n",
    "        \n",
    "        Args:\n",
    "            api_provider (str): The news API provider ('newsapi', 'finnhub')\n",
    "            api_key (str): API key for the provider\n",
    "        \"\"\"\n",
    "        self.api_provider = api_provider.lower()\n",
    "        self.api_key = api_key\n",
    "        \n",
    "        # Base URLs for news APIs\n",
    "        self.base_urls = {\n",
    "            'newsapi': 'https://newsapi.org/v2',\n",
    "            'finnhub': 'https://finnhub.io/api/v1'\n",
    "        }\n",
    "        \n",
    "        # Keywords mapping for each asset to use in news search\n",
    "        self.asset_keywords = {\n",
    "            'US100': ['NASDAQ', 'NASDAQ 100', 'NDX', 'tech stocks', 'technology sector'],\n",
    "            'US30': ['Dow Jones', 'Dow 30', 'DJIA', 'Dow Jones Industrial Average'],\n",
    "            'EUR/USD': ['EUR/USD', 'euro dollar', 'euro forex', 'euro currency', 'ECB', 'Federal Reserve'],\n",
    "            'GBP/USD': ['GBP/USD', 'pound dollar', 'sterling', 'Bank of England', 'Brexit'],\n",
    "            'Crude Oil WTI': ['crude oil', 'WTI', 'oil prices', 'OPEC', 'oil market'],\n",
    "            'Crude Oil Brent': ['Brent crude', 'Brent oil', 'oil prices', 'OPEC', 'oil market']\n",
    "        }\n",
    "\n",
    "    def get_news_for_asset(self, asset, days_back=3, max_articles=10):\n",
    "        \"\"\"\n",
    "        Fetch news articles related to a specific asset\n",
    "        \n",
    "        Args:\n",
    "            asset (str): The asset name from our standard list\n",
    "            days_back (int): Number of days to look back for news\n",
    "            max_articles (int): Maximum number of articles to fetch\n",
    "            \n",
    "        Returns:\n",
    "            list: List of news articles\n",
    "        \"\"\"\n",
    "        if self.api_provider not in self.base_urls:\n",
    "            raise ValueError(f\"Unsupported API provider: {self.api_provider}\")\n",
    "            \n",
    "        keywords = self.asset_keywords.get(asset)\n",
    "        if not keywords:\n",
    "            raise ValueError(f\"No keywords defined for asset: {asset}\")\n",
    "            \n",
    "        from_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')\n",
    "        \n",
    "        articles = []\n",
    "        \n",
    "        if self.api_provider == 'newsapi':\n",
    "            # For each keyword, get articles and merge them\n",
    "            for keyword in keywords:\n",
    "                url = f\"{self.base_urls['newsapi']}/everything\"\n",
    "                params = {\n",
    "                    'q': keyword,\n",
    "                    'from': from_date,\n",
    "                    'sortBy': 'publishedAt',\n",
    "                    'language': 'en',\n",
    "                    'apiKey': self.api_key\n",
    "                }\n",
    "                \n",
    "                try:\n",
    "                    response = requests.get(url, params=params)\n",
    "                    if response.status_code == 200:\n",
    "                        data = response.json()\n",
    "                        if data.get('status') == 'ok':\n",
    "                            articles.extend(data.get('articles', []))\n",
    "                    else:\n",
    "                        print(f\"Error fetching news from NewsAPI: {response.status_code}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Exception when fetching news: {e}\")\n",
    "                    \n",
    "        elif self.api_provider == 'finnhub':\n",
    "            # Use Finnhub's news endpoint\n",
    "            for keyword in keywords:\n",
    "                url = f\"{self.base_urls['finnhub']}/news\"\n",
    "                params = {\n",
    "                    'category': 'general',\n",
    "                    'token': self.api_key\n",
    "                }\n",
    "                \n",
    "                try:\n",
    "                    response = requests.get(url, params=params)\n",
    "                    if response.status_code == 200:\n",
    "                        data = response.json()\n",
    "                        # Filter articles containing our keyword\n",
    "                        for article in data:\n",
    "                            if keyword.lower() in article.get('headline', '').lower() or \\\n",
    "                               keyword.lower() in article.get('summary', '').lower():\n",
    "                                articles.append({\n",
    "                                    'title': article.get('headline'),\n",
    "                                    'description': article.get('summary'),\n",
    "                                    'url': article.get('url'),\n",
    "                                    'publishedAt': article.get('datetime'),\n",
    "                                    'source': {'name': article.get('source')}\n",
    "                                })\n",
    "                except Exception as e:\n",
    "                    print(f\"Exception when fetching news: {e}\")\n",
    "        \n",
    "        # Deduplicate articles based on title\n",
    "        unique_articles = []\n",
    "        titles = set()\n",
    "        for article in articles:\n",
    "            if article['title'] not in titles:\n",
    "                titles.add(article['title'])\n",
    "                unique_articles.append(article)\n",
    "                \n",
    "                # Add the asset reference\n",
    "                article['asset'] = asset\n",
    "        \n",
    "        # Sort by publication date and return the most recent ones\n",
    "        sorted_articles = sorted(unique_articles, \n",
    "                                 key=lambda x: x.get('publishedAt', ''),\n",
    "                                 reverse=True)\n",
    "        \n",
    "        return sorted_articles[:max_articles]\n",
    "\n",
    "class SentimentAnalyzer:\n",
    "    \"\"\"\n",
    "    A class for analyzing sentiment of financial news articles\n",
    "    using both VADER and TextBlob\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.vader = SentimentIntensityAnalyzer()\n",
    "        \n",
    "    def analyze_text(self, text):\n",
    "        \"\"\"\n",
    "        Analyze sentiment of a text using both VADER and TextBlob\n",
    "        \n",
    "        Args:\n",
    "            text (str): The text to analyze\n",
    "            \n",
    "        Returns:\n",
    "            dict: Sentiment scores from both analyzers\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return {\n",
    "                'vader_compound': 0,\n",
    "                'vader_pos': 0,\n",
    "                'vader_neg': 0,\n",
    "                'vader_neu': 0,\n",
    "                'textblob_polarity': 0,\n",
    "                'textblob_subjectivity': 0,\n",
    "                'sentiment_label': 'neutral'\n",
    "            }\n",
    "        \n",
    "        # VADER sentiment analysis\n",
    "        vader_scores = self.vader.polarity_scores(text)\n",
    "        \n",
    "        # TextBlob sentiment analysis\n",
    "        tb = TextBlob(text)\n",
    "        textblob_polarity = tb.sentiment.polarity\n",
    "        textblob_subjectivity = tb.sentiment.subjectivity\n",
    "        \n",
    "        # Combined sentiment label\n",
    "        # Use VADER's compound score as the primary indicator\n",
    "        if vader_scores['compound'] >= 0.05:\n",
    "            sentiment_label = 'positive'\n",
    "        elif vader_scores['compound'] <= -0.05:\n",
    "            sentiment_label = 'negative'\n",
    "        else:\n",
    "            sentiment_label = 'neutral'\n",
    "        \n",
    "        return {\n",
    "            'vader_compound': vader_scores['compound'],\n",
    "            'vader_pos': vader_scores['pos'],\n",
    "            'vader_neg': vader_scores['neg'],\n",
    "            'vader_neu': vader_scores['neu'],\n",
    "            'textblob_polarity': textblob_polarity,\n",
    "            'textblob_subjectivity': textblob_subjectivity,\n",
    "            'sentiment_label': sentiment_label\n",
    "        }\n",
    "    \n",
    "    def analyze_article(self, article):\n",
    "        \"\"\"\n",
    "        Analyze sentiment of a news article\n",
    "        \n",
    "        Args:\n",
    "            article (dict): News article dictionary\n",
    "            \n",
    "        Returns:\n",
    "            dict: Original article with added sentiment analysis\n",
    "        \"\"\"\n",
    "        # Create a combined text from title and description\n",
    "        title = article.get('title', '')\n",
    "        description = article.get('description', '')\n",
    "        combined_text = f\"{title}. {description}\"\n",
    "        \n",
    "        # Get sentiment scores\n",
    "        sentiment_scores = self.analyze_text(combined_text)\n",
    "        \n",
    "        # Add to the article dictionary\n",
    "        article.update({\n",
    "            'sentiment': sentiment_scores,\n",
    "            'analyzed_at': datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "        return article\n",
    "    \n",
    "    def extract_key_phrases(self, text, max_phrases=5):\n",
    "        \"\"\"\n",
    "        Extract key phrases from text\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text\n",
    "            max_phrases (int): Maximum number of phrases to extract\n",
    "            \n",
    "        Returns:\n",
    "            list: List of key phrases\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "            \n",
    "        # Simple noun phrase extraction\n",
    "        tb = TextBlob(text)\n",
    "        noun_phrases = list(tb.noun_phrases)\n",
    "        \n",
    "        # Simple keyword extraction based on POS tagging\n",
    "        important_tags = ['NN', 'NNS', 'NNP', 'NNPS', 'JJ']\n",
    "        words = tb.tags  # Get words with POS tags\n",
    "        keywords = [word for word, tag in words if tag in important_tags]\n",
    "        \n",
    "        # Combine phrases and keywords, remove duplicates\n",
    "        all_phrases = noun_phrases + keywords\n",
    "        unique_phrases = list(set(all_phrases))\n",
    "        \n",
    "        # Sort by length (favor longer phrases) and return top N\n",
    "        sorted_phrases = sorted(unique_phrases, key=len, reverse=True)\n",
    "        \n",
    "        return sorted_phrases[:max_phrases]\n",
    "\n",
    "class NewsSentimentManager:\n",
    "    \"\"\"\n",
    "    Manages the collection and analysis of news sentiment for trading assets\n",
    "    \"\"\"\n",
    "    def __init__(self, news_api_client, sentiment_analyzer):\n",
    "        self.news_api_client = news_api_client\n",
    "        self.sentiment_analyzer = sentiment_analyzer\n",
    "        self.data_dir = os.path.join(news_dir, 'data')\n",
    "        os.makedirs(self.data_dir, exist_ok=True)\n",
    "        \n",
    "    def collect_and_analyze_news(self, assets, days_back=3, max_articles_per_asset=10):\n",
    "        \"\"\"\n",
    "        Collect and analyze news for multiple assets\n",
    "        \n",
    "        Args:\n",
    "            assets (list): List of asset names\n",
    "            days_back (int): Days to look back for news\n",
    "            max_articles_per_asset (int): Max articles per asset\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary with asset names as keys and analyzed articles as values\n",
    "        \"\"\"\n",
    "        all_results = {}\n",
    "        \n",
    "        for asset in assets:\n",
    "            print(f\"Collecting news for {asset}...\")\n",
    "            \n",
    "            # Get news articles\n",
    "            articles = self.news_api_client.get_news_for_asset(\n",
    "                asset, days_back, max_articles_per_asset\n",
    "            )\n",
    "            \n",
    "            # Analyze sentiment\n",
    "            analyzed_articles = []\n",
    "            for article in articles:\n",
    "                analyzed_article = self.sentiment_analyzer.analyze_article(article)\n",
    "                analyzed_articles.append(analyzed_article)\n",
    "            \n",
    "            all_results[asset] = analyzed_articles\n",
    "            \n",
    "            # Save to json file\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"{asset.replace('/', '_')}_{timestamp}.json\"\n",
    "            with open(os.path.join(self.data_dir, filename), 'w') as f:\n",
    "                json.dump(analyzed_articles, f, indent=4)\n",
    "                \n",
    "        return all_results\n",
    "    \n",
    "    def calculate_asset_sentiment_summary(self, asset_articles):\n",
    "        \"\"\"\n",
    "        Calculate summary sentiment metrics for an asset\n",
    "        \n",
    "        Args:\n",
    "            asset_articles (list): List of analyzed articles for an asset\n",
    "            \n",
    "        Returns:\n",
    "            dict: Summary sentiment metrics\n",
    "        \"\"\"\n",
    "        if not asset_articles:\n",
    "            return {\n",
    "                'count': 0,\n",
    "                'avg_vader_compound': 0,\n",
    "                'avg_textblob_polarity': 0,\n",
    "                'positive_count': 0,\n",
    "                'negative_count': 0,\n",
    "                'neutral_count': 0,\n",
    "                'sentiment_distribution': {\n",
    "                    'positive': 0,\n",
    "                    'neutral': 0,\n",
    "                    'negative': 0\n",
    "                },\n",
    "                'overall_sentiment': 'neutral',\n",
    "                'key_phrases': []\n",
    "            }\n",
    "            \n",
    "        # Extract sentiment scores\n",
    "        vader_scores = [a['sentiment']['vader_compound'] for a in asset_articles]\n",
    "        textblob_scores = [a['sentiment']['textblob_polarity'] for a in asset_articles]\n",
    "        \n",
    "        # Count sentiments\n",
    "        sentiments = [a['sentiment']['sentiment_label'] for a in asset_articles]\n",
    "        positive_count = sentiments.count('positive')\n",
    "        negative_count = sentiments.count('negative')\n",
    "        neutral_count = sentiments.count('neutral')\n",
    "        total_count = len(sentiments)\n",
    "        \n",
    "        # Calculate distribution percentages\n",
    "        sentiment_dist = {\n",
    "            'positive': (positive_count / total_count) * 100 if total_count > 0 else 0,\n",
    "            'neutral': (neutral_count / total_count) * 100 if total_count > 0 else 0, \n",
    "            'negative': (negative_count / total_count) * 100 if total_count > 0 else 0\n",
    "        }\n",
    "        \n",
    "        # Determine overall sentiment\n",
    "        if positive_count > negative_count and positive_count > neutral_count:\n",
    "            overall = 'positive'\n",
    "        elif negative_count > positive_count and negative_count > neutral_count:\n",
    "            overall = 'negative'\n",
    "        else:\n",
    "            overall = 'neutral'\n",
    "            \n",
    "        # Extract key phrases across all articles\n",
    "        all_text = \" \".join([\n",
    "            f\"{a.get('title', '')}. {a.get('description', '')}\" \n",
    "            for a in asset_articles\n",
    "        ])\n",
    "        key_phrases = self.sentiment_analyzer.extract_key_phrases(all_text, 10)\n",
    "        \n",
    "        return {\n",
    "            'count': total_count,\n",
    "            'avg_vader_compound': np.mean(vader_scores) if vader_scores else 0,\n",
    "            'avg_textblob_polarity': np.mean(textblob_scores) if textblob_scores else 0,\n",
    "            'positive_count': positive_count,\n",
    "            'negative_count': negative_count,\n",
    "            'neutral_count': neutral_count,\n",
    "            'sentiment_distribution': sentiment_dist,\n",
    "            'overall_sentiment': overall,\n",
    "            'key_phrases': key_phrases\n",
    "        }\n",
    "    \n",
    "    def get_news_sentiment_features(self, all_assets_articles):\n",
    "        \"\"\"\n",
    "        Extract sentiment features for ML model\n",
    "        \n",
    "        Args:\n",
    "            all_assets_articles (dict): Dictionary with asset names and analyzed articles\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary with asset names and sentiment features\n",
    "        \"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        for asset, articles in all_assets_articles.items():\n",
    "            # Get summary\n",
    "            summary = self.calculate_asset_sentiment_summary(articles)\n",
    "            \n",
    "            # Extract ML features\n",
    "            features[asset] = {\n",
    "                'sentiment_score': summary['avg_vader_compound'],  # Main sentiment score\n",
    "                'sentiment_magnitude': abs(summary['avg_vader_compound']),  # Intensity of sentiment\n",
    "                'textblob_score': summary['avg_textblob_polarity'],\n",
    "                'positive_ratio': summary['sentiment_distribution']['positive'] / 100,\n",
    "                'negative_ratio': summary['sentiment_distribution']['negative'] / 100,\n",
    "                'neutral_ratio': summary['sentiment_distribution']['neutral'] / 100,\n",
    "                'article_count': summary['count'],\n",
    "                'sentiment_label': summary['overall_sentiment'],\n",
    "                'has_news': summary['count'] > 0\n",
    "            }\n",
    "            \n",
    "        return features\n",
    "\n",
    "# Create example usage file to demonstrate how to use the sentiment analysis module\n",
    "with open(os.path.join(news_dir, 'example_usage.py'), 'w') as f:\n",
    "    f.write(\"\"\"import os\n",
    "import json\n",
    "from news_api_client import NewsAPIClient\n",
    "from sentiment_analyzer import SentimentAnalyzer\n",
    "from news_sentiment_manager import NewsSentimentManager\n",
    "\n",
    "# Initialize components\n",
    "news_client = NewsAPIClient(api_provider='newsapi', api_key='YOUR_API_KEY')\n",
    "sentiment_analyzer = SentimentAnalyzer()\n",
    "manager = NewsSentimentManager(news_client, sentiment_analyzer)\n",
    "\n",
    "# Define assets to analyze\n",
    "assets = ['US100', 'US30', 'EUR/USD', 'GBP/USD', 'Crude Oil WTI', 'Crude Oil Brent']\n",
    "\n",
    "# Collect and analyze news\n",
    "results = manager.collect_and_analyze_news(assets, days_back=3, max_articles_per_asset=5)\n",
    "\n",
    "# Get sentiment features for ML model\n",
    "sentiment_features = manager.get_news_sentiment_features(results)\n",
    "\n",
    "# Print summary for each asset\n",
    "for asset, articles in results.items():\n",
    "    summary = manager.calculate_asset_sentiment_summary(articles)\n",
    "    print(f\"\\\\nSentiment Summary for {asset}:\")\n",
    "    print(f\"Number of articles: {summary['count']}\")\n",
    "    print(f\"Average sentiment score: {summary['avg_vader_compound']:.2f}\")\n",
    "    print(f\"Sentiment distribution: {summary['sentiment_distribution']}\")\n",
    "    print(f\"Overall sentiment: {summary['overall_sentiment']}\")\n",
    "    print(f\"Key phrases: {', '.join(summary['key_phrases'][:3])}\")\n",
    "\"\"\")\n",
    "\n",
    "# Create a README file for the news sentiment module with complete content\n",
    "readme_content = \"\"\"# News Sentiment Analysis Module\n",
    "\n",
    "This module handles the collection and analysis of financial news sentiment for trading assets.\n",
    "\n",
    "## Components\n",
    "\n",
    "- `NewsAPIClient`: Fetches news articles from various news API providers\n",
    "- `SentimentAnalyzer`: Analyzes sentiment using VADER and TextBlob\n",
    "- `NewsSentimentManager`: Orchestrates collection, analysis, and storage of sentiment data\n",
    "\n",
    "## Supported Assets\n",
    "\n",
    "- US100 (NASDAQ 100)\n",
    "- US30 (Dow Jones Industrial Average)\n",
    "- EUR/USD (Euro/US Dollar)\n",
    "- GBP/USD (British Pound/US Dollar)\n",
    "- Crude Oil WTI\n",
    "- Crude Oil Brent\n",
    "\n",
    "## Usage\n",
    "\n",
    "### Basic Usage Example\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
